{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "from pims import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from skimage.io import imread\n",
    "import imageio\n",
    "from glob import glob\n",
    "from cellocity.channel import Channel\n",
    "import tifffile\n",
    "from tifffile import imshow\n",
    "from time import time\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import dabest\n",
    "import seaborn as sns\n",
    "from skimage.filters import difference_of_gaussians, window\n",
    "from scipy.fft import fftn, fftshift\n",
    "import plotly\n",
    "from scipy import stats\n",
    "from skimage import exposure#, filters, util\n",
    "# import skimage\n",
    "from openpiv import preprocess as piv_pre\n",
    "import openpiv.tools as piv_tls\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import builtins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpiv import tools, pyprocess, validation, filters, scaling, piv, preprocess\n",
    "from openpiv.pyprocess import *\n",
    "import openpiv.pyprocess as process\n",
    "from openpiv import pyprocess\n",
    "from openpiv.pyprocess import extended_search_area_piv, get_field_shape, get_coordinates\n",
    "from openpiv import smoothn\n",
    "from openpiv.preprocess import mask_coordinates\n",
    "from openpiv.piv import simple_piv\n",
    "from openpiv.tools import imread, Multiprocesser, display_vector_field, \\\n",
    "    transform_coordinates\n",
    "from openpiv import validation, filters, tools, preprocess, scaling, tools\n",
    "from skimage.measure import points_in_poly\n",
    "from skimage.util import invert\n",
    "from openpiv import windef\n",
    "from openpiv.windef import first_pass, multipass_img_deform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_run(frame_a, frame_b, mask_a, mask_b, settings, file_name, counter):\n",
    "    # \"first pass\"\n",
    "    x, y, u, v, s2n = first_pass(frame_a, frame_b, settings)\n",
    "\n",
    "    if settings.show_all_plots:\n",
    "        plt.figure()\n",
    "        plt.quiver(x,y,u,-v,color='b')\n",
    "        # plt.gca().invert_yaxis()\n",
    "        # plt.gca().set_aspect(1.)\n",
    "        # plt.title('after first pass, invert')\n",
    "        # plt.show()\n",
    "\n",
    "    # \" Image masking \"\n",
    "    if settings.image_mask:\n",
    "        image_mask = np.logical_and(mask_a, mask_b)\n",
    "        mask_coords = preprocess.mask_coordinates(image_mask)\n",
    "        # mark those points on the grid of PIV inside the mask\n",
    "        grid_mask = preprocess.prepare_mask_on_grid(x,y,mask_coords)\n",
    "\n",
    "        # mask the velocity\n",
    "        u = np.ma.masked_array(u, mask=grid_mask)\n",
    "        v = np.ma.masked_array(v, mask=grid_mask)\n",
    "    else:\n",
    "        mask_coords = []\n",
    "        u = np.ma.masked_array(u, mask=np.ma.nomask)\n",
    "        v = np.ma.masked_array(v, mask=np.ma.nomask)\n",
    "\n",
    "    if settings.validation_first_pass:\n",
    "        u, v, mask = validation.typical_validation(u, v, s2n, settings)\n",
    "\n",
    "    if settings.show_all_plots:\n",
    "        plt.figure()\n",
    "        plt.quiver(x,y,u,-v,color='r')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_aspect(1.)\n",
    "        plt.title('after first pass validation new, inverted')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # \"filter to replace the values that where marked by the validation\"\n",
    "    if settings.num_iterations == 1 and settings.replace_vectors:\n",
    "        # for multi-pass we cannot have holes in the data\n",
    "        # after the first pass\n",
    "        u, v = filters.replace_outliers(\n",
    "            u,\n",
    "            v,\n",
    "            method=settings.filter_method,\n",
    "            max_iter=settings.max_filter_iteration,\n",
    "            kernel_size=settings.filter_kernel_size,\n",
    "        )\n",
    "    elif settings.num_iterations > 1: # don't even check if it's true or false\n",
    "        u, v = filters.replace_outliers(\n",
    "            u,\n",
    "            v,\n",
    "            method=settings.filter_method,\n",
    "            max_iter=settings.max_filter_iteration,\n",
    "            kernel_size=settings.filter_kernel_size,\n",
    "        )\n",
    "\n",
    "        # \"adding masks to add the effect of all the validations\"\n",
    "    if settings.smoothn:\n",
    "        u, dummy_u1, dummy_u2, dummy_u3 = smoothn.smoothn(u, s=settings.smoothn_p)\n",
    "        v, dummy_v1, dummy_v2, dummy_v3 = smoothn.smoothn(v, s=settings.smoothn_p)\n",
    "\n",
    "    if settings.image_mask:\n",
    "        grid_mask = preprocess.prepare_mask_on_grid(x, y, mask_coords)\n",
    "        u = np.ma.masked_array(u, mask=grid_mask)\n",
    "        v = np.ma.masked_array(v, mask=grid_mask)\n",
    "    else:\n",
    "        u = np.ma.masked_array(u, np.ma.nomask)\n",
    "        v = np.ma.masked_array(v, np.ma.nomask)\n",
    "\n",
    "\n",
    "    if settings.show_all_plots:\n",
    "        plt.figure()\n",
    "        plt.quiver(x,y,u,-v)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_aspect(1.)\n",
    "        plt.title('before multi pass, inverted')\n",
    "        plt.show()\n",
    "\n",
    "    if not isinstance(u, np.ma.MaskedArray):\n",
    "        raise ValueError(\"Expected masked array\")\n",
    "\n",
    "    \"\"\" Multi pass \"\"\"\n",
    "    for i in range(1, settings.num_iterations):\n",
    "        if not isinstance(u, np.ma.MaskedArray):\n",
    "            raise ValueError(\"Expected masked array\")\n",
    "        x, y, u, v, s2n, mask = multipass_img_deform(\n",
    "            frame_a,\n",
    "            frame_b,\n",
    "            i,\n",
    "            x,\n",
    "            y,\n",
    "            u,\n",
    "            v,\n",
    "            settings,\n",
    "            mask_coords=mask_coords\n",
    "        )   \n",
    "        \n",
    "        # If the smoothing is active, we do it at each pass\n",
    "        # but not the last one\n",
    "        if settings.smoothn is True and i < settings.num_iterations-1: \n",
    "            u, dummy_u1, dummy_u2, dummy_u3 = smoothn.smoothn(u, s=settings.smoothn_p)\n",
    "            v, dummy_v1, dummy_v2, dummy_v3 = smoothn.smoothn(v, s=settings.smoothn_p)\n",
    "        if not isinstance(u, np.ma.MaskedArray):\n",
    "            raise ValueError ('not a masked array anymore')\n",
    "\n",
    "        if hasattr(settings, 'image_mask') and settings.image_mask:\n",
    "            grid_mask = preprocess.prepare_mask_on_grid(x, y, mask_coords)\n",
    "            u = np.ma.masked_array(u, mask=grid_mask)\n",
    "            v = np.ma.masked_array(v, mask=grid_mask)\n",
    "        else:\n",
    "            u = np.ma.masked_array(u, np.ma.nomask)\n",
    "            v = np.ma.masked_array(v, np.ma.nomask)\n",
    "\n",
    "        if settings.show_all_plots:\n",
    "            plt.figure()\n",
    "            plt.quiver(x, y, u, -v, color='r')\n",
    "            plt.gca().set_aspect(1.)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.title('end of the multipass, invert')\n",
    "            plt.show()\n",
    "\n",
    "    if settings.show_all_plots and settings.num_iterations > 1:\n",
    "        plt.figure()\n",
    "        plt.quiver(x,y,u,-v)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_aspect(1.)\n",
    "        plt.title('after multi pass, before saving, inverted')\n",
    "        plt.show()\n",
    "\n",
    "    # we now use only 0s instead of the image\n",
    "    # masked regions. \n",
    "    # we could do Nan, not sure what is best\n",
    "    u = u.filled(0.)\n",
    "    v = v.filled(0.)\n",
    "\n",
    "    # \"scales the results pixel-> meter\"\n",
    "    x, y, u, v = scaling.uniform(x, y, u, v, scaling_factor=settings.scaling_factor)\n",
    "\n",
    "    if settings.image_mask:\n",
    "        grid_mask = preprocess.prepare_mask_on_grid(x, y, mask_coords)\n",
    "        u = np.ma.masked_array(u, mask=grid_mask)\n",
    "        v = np.ma.masked_array(v, mask=grid_mask)\n",
    "    else:\n",
    "        u = np.ma.masked_array(u, np.ma.nomask)\n",
    "        v = np.ma.masked_array(v, np.ma.nomask)\n",
    "\n",
    "    # before saving we conver to the \"physically relevant\"\n",
    "    # right-hand coordinate system with 0,0 at the bottom left\n",
    "    # x to the right, y upwards\n",
    "    # and so u,v\n",
    "\n",
    "    x, y, u, v = transform_coordinates(x, y, u, v)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # \"save to a file\"\n",
    "    tools.save(x, y, u, v, mask,\n",
    "        os.path.join(settings.save_path, \"field_%s_%03d.txt\" %(file_name,counter)),\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    \n",
    "        # \"some other stuff that one might want to use\"\n",
    "    if settings.show_plot or settings.save_plot:\n",
    "        Name = os.path.join(settings.save_path, \"PIV_%s_%03d.png\" %(file_name,counter))\n",
    "        fig, _ = display_vector_field(\n",
    "            os.path.join(settings.save_path, \"field_%s_%03d.txt\" %(file_name,counter)),\n",
    "            scale=settings.scale_plot,\n",
    "        )\n",
    "        if settings.save_plot is True:\n",
    "            fig.savefig(Name)\n",
    "        if settings.show_plot is True:\n",
    "            plt.show()\n",
    "    return x, y, u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv(settings, file_name, mask_a, mask_b, counter):\n",
    "    \"\"\"A function to process each image pair.\"\"\"\n",
    "\n",
    "    frame_a = settings.frame_pattern_a\n",
    "    frame_b = settings.frame_pattern_b\n",
    "\n",
    "    \" crop to ROI\"\n",
    "    if settings.ROI == \"full\":\n",
    "        frame_a = frame_a\n",
    "        frame_b = frame_b\n",
    "    else:\n",
    "        frame_a = frame_a[\n",
    "            settings.ROI[0]:settings.ROI[1],\n",
    "            settings.ROI[2]:settings.ROI[3]\n",
    "        ]\n",
    "        frame_b = frame_b[\n",
    "            settings.ROI[0]:settings.ROI[1],\n",
    "            settings.ROI[2]:settings.ROI[3]\n",
    "        ]\n",
    "\n",
    "    if settings.invert is True:\n",
    "        frame_a = invert(frame_a)\n",
    "        frame_b = invert(frame_b)\n",
    "\n",
    "    if settings.show_all_plots:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.imshow(frame_a, cmap=plt.get_cmap('Reds'))\n",
    "        ax.imshow(frame_b, cmap=plt.get_cmap('Blues'),alpha=.5)\n",
    "        plt.show()\n",
    "\n",
    "    if settings.dynamic_masking_method in (\"edge\", \"intensity\"):\n",
    "        frame_a, mask_a = preprocess.dynamic_masking(\n",
    "            frame_a,\n",
    "            method=settings.dynamic_masking_method,\n",
    "            filter_size=settings.dynamic_masking_filter_size,\n",
    "            threshold=settings.dynamic_masking_threshold,\n",
    "        )\n",
    "        frame_b, mask_b = preprocess.dynamic_masking(\n",
    "            frame_b,\n",
    "            method=settings.dynamic_masking_method,\n",
    "            filter_size=settings.dynamic_masking_filter_size,\n",
    "            threshold=settings.dynamic_masking_threshold,\n",
    "        )\n",
    "\n",
    "    x, y, u, v = piv_run(frame_a, frame_b, mask_a, mask_b, settings, file_name, counter)    \n",
    "    return x, y, u, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = windef.Settings()\n",
    "\n",
    "'Region of interest'\n",
    "settings.ROI = 'full'\n",
    "\n",
    "'Image preprocessing'\n",
    "# 'None' for no masking, 'edges' for edges masking, 'intensity' for intensity masking\n",
    "settings.dynamic_masking_method = 'edges'\n",
    "settings.dynamic_masking_threshold = 0.5\n",
    "settings.dynamic_masking_filter_size = 21\n",
    "\n",
    "# settings.deformation_method = 'symmetric' \n",
    "settings.deformation_method = 'second image'\n",
    "\n",
    "'Processing Parameters'\n",
    "settings.correlation_method='linear'  # 'circular' or 'linear'\n",
    "settings.normalized_correlation = True\n",
    "\n",
    "settings.num_iterations = 4  # select the number of PIV passes\n",
    "settings.windowsizes=(64, 64, 32, 32, 16, 16)\n",
    "settings.overlap=(32, 32, 16, 16, 8, 8)\n",
    "\n",
    "# Has to be a value with base two. In general window size/2 is a good choice.\n",
    "# method used for subpixel interpolation: 'gaussian','centroid','parabolic'\n",
    "settings.subpixel_method = 'gaussian'\n",
    "\n",
    "# order of the image interpolation for the window deformation\n",
    "settings.interpolation_order = 3\n",
    "settings.scaling_factor = 1  # scaling factor pixel/meter\n",
    "settings.dt = 1  # time between to frames (in seconds)\n",
    "\n",
    "'Signal to noise ratio options (only for the last pass)'\n",
    "# It is possible to decide if the S/N should be computed (for the last pass) or not\n",
    "# settings.extract_sig2noise = True  # 'True' or 'False' (only for the last pass)\n",
    "settings.sig2noise_threshold = 1.3\n",
    "# method used to calculate the signal to noise ratio 'peak2peak' or 'peak2mean'\n",
    "settings.sig2noise_method = 'peak2mean'\n",
    "# select the width of the masked to masked out pixels next to the main peak\n",
    "settings.sig2noise_mask = 2\n",
    "settings.sig2noise_validate = False\n",
    "\n",
    "'vector validation options'\n",
    "# choose if you want to do validation of the first pass: True or False\n",
    "settings.validation_first_pass = True\n",
    "\n",
    "'Validation Parameters'\n",
    "# The validation is done at each iteration based on three filters.\n",
    "# The first filter is based on the min/max ranges. Observe that these values are defined in\n",
    "# terms of minimum and maximum displacement in pixel/frames.\n",
    "settings.MinMax_U_disp = (-5, 5)\n",
    "settings.MinMax_V_disp = (-5, 5)\n",
    "# The second filter is based on the global STD threshold\n",
    "settings.std_threshold = 2  # threshold of the std validation\n",
    "# The third filter is the median test (not normalized at the moment)\n",
    "settings.median_threshold = 2  # threshold of the median validation\n",
    "# On the last iteration, an additional validation can be done based on the S/N.\n",
    "settings.median_size = 2 # defines the size of the local median\n",
    "\n",
    "'Outlier replacement or Smoothing options'\n",
    "# Replacment options for vectors which are masked as invalid by the validation\n",
    "settings.replace_vectors = True # Enable the replacment. Chosse: True or False\n",
    "settings.smoothn = True #Enables smoothing of the displacemenet field\n",
    "settings.smoothn_p = 0.5 # This is a smoothing parameter\n",
    "# select a method to replace the outliers: 'localmean', 'disk', 'distance'\n",
    "settings.filter_method = 'localmean'\n",
    "# maximum iterations performed to replace the outliers\n",
    "settings.max_filter_iteration = 1\n",
    "settings.filter_kernel_size = 1  # kernel size for the localmean method\n",
    "\n",
    "'Output options'\n",
    "# Select if you want to save the plotted vectorfield: True or False\n",
    "settings.save_plot = False\n",
    "# Choose wether you want to see the vectorfield or not :True or False\n",
    "settings.show_plot = False\n",
    "settings.scale_plot = 20  # select a value to scale the quiver plot of the vectorfield\n",
    "\n",
    "settings.image_mask = True\n",
    "settings.show_all_plots = False\n",
    "settings.invert = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the variability in velocity direction by quantifying angular deviation\n",
    "# angular deviation is calculated as 2(√1−z) where z=1/N[(∑cosθ)^2+(∑sinθ)^2]1/2\n",
    "# function takes a set of angles as input\n",
    "# function returns array with angular deviation of array\n",
    "\n",
    "def calc_ang_dev(angles):\n",
    "    non_nan = np.array(angles)[~np.isnan(angles)]\n",
    "    term1 = sum(np.cos(non_nan))/np.size(non_nan)\n",
    "    term2 = sum(np.sin(non_nan))/np.size(non_nan)\n",
    "    z = np.sqrt(np.square(term1)+np.square(term2))\n",
    "    ang_dev = np.sqrt(2*(1-z))\n",
    "    return ang_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ang_btwn(p1, p2):\n",
    "    dx=p2[0]-p1[0]\n",
    "    dy=p2[1]-p1[1]\n",
    "    return np.arctan2(dy,dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def straighten_scratch(binary_image):\n",
    "    cnt = cv.findContours(~binary_image[0], cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)[0]\n",
    "    for k in range(len(cnt)):\n",
    "        area = cv.contourArea(cnt[k])\n",
    "        if area>2000:\n",
    "            # grab the (x, y) coordinates of all border values\n",
    "            otln = np.squeeze(cnt[k])\n",
    "            min_x = int(min(otln.T[0]))\n",
    "            min_y = int(min(otln.T[1]))\n",
    "            max_x = int(max(otln.T[0]))\n",
    "            max_y = int(max(otln.T[1]))\n",
    "            dl=[]\n",
    "            for j in range(len(otln)):\n",
    "                if otln.tolist()[j][0] == min_x or otln.tolist()[j][0] == max_x or otln.tolist()[j][1] == min_y or otln.tolist()[j][1] == max_y:\n",
    "                    dl.append(j)\n",
    "\n",
    "            # We will than be deleting these values from our contour array so that we are only\n",
    "            # looking at the monolayer edge boundarys\n",
    "            rslt = np.delete(otln, dl, 0)\n",
    "\n",
    "            # compute a rotated bounding box that contains all coordinates\n",
    "            box_ang  = cv.minAreaRect(rslt)[-1]\n",
    "\n",
    "            # the `cv2.minAreaRect` function returns values in the\n",
    "            # range [-90, 0); as the rectangle rotates clockwise the\n",
    "            # returned angle trends to 0\n",
    "            if box_ang > 45:\n",
    "                box_ang = (90 - box_ang)\n",
    "            # otherwise, just take the inverse of the angle to make it positive\n",
    "            else:\n",
    "                box_ang = -box_ang\n",
    "#             box_ang = np.deg2rad(box_ang)\n",
    "    return box_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_matrix (x, y, angle, x_shift=0, y_shift=0, units=\"DEGREES\"):\n",
    "    \"\"\"\n",
    "    Rotates a point in the xy-plane counterclockwise through an angle about the origin\n",
    "    https://en.wikipedia.org/wiki/Rotation_matrix\n",
    "    :param x: x coordinate\n",
    "    :param y: y coordinate\n",
    "    :param x_shift: x-axis shift from origin (0, 0)\n",
    "    :param y_shift: y-axis shift from origin (0, 0)\n",
    "    :param angle: The rotation angle in degrees\n",
    "    :param units: DEGREES (default) or RADIANS\n",
    "    :return: Tuple of rotated x and y\n",
    "    \"\"\"\n",
    "\n",
    "    # Shift to origin (0,0)\n",
    "    x = x - x_shift\n",
    "    y = y - y_shift\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    if units == \"DEGREES\":\n",
    "        angle = math.radians(angle)\n",
    "\n",
    "    # Rotation matrix multiplication to get rotated x & y\n",
    "    xr = (x * math.cos(angle)) - (y * math.sin(angle)) + x_shift\n",
    "    yr = (x * math.sin(angle)) + (y * math.cos(angle)) + y_shift\n",
    "\n",
    "    return xr, yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cohens d using the function defined here: https://machinelearningmastery.com/effect-size-measures-in-python/\n",
    "def cohend(d1, d2):\n",
    "    # calculate the size of samples\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    # calculate the variance of the samples\n",
    "    s1, s2 = np.nanvar(d1, ddof=1), np.nanvar(d2, ddof=1)\n",
    "    # calculate the pooled standard deviation\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    # calculate the means of the samples\n",
    "    u1, u2 = np.nanmean(d1), np.nanmean(d2)\n",
    "    # calculate the effect size\n",
    "    return (u2 - u1) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dists(X, Y, r_bin_size):\n",
    "    # Calculate all possible distances within a FOV\n",
    "    XY = np.dstack((X[0],Y[0]))\n",
    "    XY_flat = XY.reshape(-1, XY.shape[-1])\n",
    "    a, b, c = np.shape(XY)\n",
    "    dists = []\n",
    "\n",
    "    for i in range(a):\n",
    "        cols = []\n",
    "        for j in range(b):\n",
    "            point = XY[i, j]\n",
    "            shp = []\n",
    "            for k in range(len(XY_flat)):\n",
    "                pnt_dist = np.linalg.norm(XY_flat[k]-point)\n",
    "                shp.append(pnt_dist)\n",
    "            taco = np.reshape(shp, (a,b))\n",
    "            cols.append(taco)\n",
    "        dists.append(cols)\n",
    "\n",
    "    max_dist = min(np.nanmax(dists), np.inf)\n",
    "    r_bins = np.arange(0, max_dist, r_bin_size)\n",
    "    \n",
    "    return dists, r_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_coord_variability(v_rho, X_scaled, Y_scaled, r_bin_size):\n",
    "    'Calculate all possible distances within a FOV'\n",
    "    dists, r_bins = get_dists(X_scaled, Y_scaled, r_bin_size)\n",
    "    \n",
    "    Cr_jj = []\n",
    "    N_jj = []\n",
    "    Cr_spatial_jj={}\n",
    "    Cr_m_jj = []\n",
    "    Cr_std_jj = []\n",
    "\n",
    "    for jj in range(len(r_bins)):\n",
    "        if jj == 1:\n",
    "            r, s, t, u = np.where(dists == r_bins[jj])\n",
    "        else:\n",
    "            r, s, t, u = np.where((dists > r_bins[jj-1]) & (dists < r_bins[jj]))\n",
    "\n",
    "        Cr_kk = []\n",
    "        N_kk = []\n",
    "        Cr_spatial_kk = []\n",
    "        Cr_m_kk = []\n",
    "        Cr_std_kk = []\n",
    "        for kk in range(len(piv_val)):\n",
    "            im = piv_val[kk]\n",
    "            im1 = []\n",
    "            im2 = []\n",
    "            for q in range(len(r)):\n",
    "                im_1 = im[r[q], s[q]]\n",
    "                im_2 = im[t[q], u[q]]\n",
    "                im1.append(im_1)\n",
    "                im2.append(im_2)\n",
    "\n",
    "            # makes sure the terms have same number of unmasked values\n",
    "            mask = np.divide(np.multiply(im1, im2), np.multiply(im1, im2))\n",
    "            im1 = np.multiply(mask, im1)\n",
    "            im2 = np.multiply(mask, im2)\n",
    "\n",
    "            corr_temp = np.multiply(im1, im2)        \n",
    "            term1 = np.square(im1)\n",
    "            term1 = np.nansum(term1)\n",
    "            term2 = np.square(im2)\n",
    "            term2 = np.nansum(term2)\n",
    "\n",
    "            Cr = np.divide(np.nansum(corr_temp), np.sqrt(np.multiply(term1, term2)))\n",
    "            N = sum(~np.isnan(corr_temp))\n",
    "            Cr_spatial = len(np.where(~np.isnan(corr_temp)))*np.divide(corr_temp, np.sqrt(np.multiply(term1,term2)))\n",
    "            Cr_m = len(np.where(~np.isnan(corr_temp)))*np.divide(np.nanmean(corr_temp), np.sqrt(np.multiply(term1, term2)))\n",
    "            Cr_std = len(np.where(~np.isnan(corr_temp)))*np.divide(np.nanstd(corr_temp), np.sqrt(np.multiply(term1,term2)))\n",
    "\n",
    "            Cr_kk.append(Cr)\n",
    "            N_kk.append(N)\n",
    "            Cr_spatial_kk.append(Cr_spatial)\n",
    "            Cr_m_kk.append(Cr_m)\n",
    "            Cr_std_kk.append(Cr_std)\n",
    "\n",
    "        Cr_jj.append(Cr_kk)\n",
    "        N_jj.append(N_kk)\n",
    "        Cr_spatial_jj.update({jj:Cr_spatial_kk})\n",
    "        Cr_m_jj.append(Cr_m_kk)\n",
    "        Cr_std_jj.append(Cr_std_kk)\n",
    "\n",
    "    return(r_bins, Cr_jj, N_jj, Cr_spatial_jj, Cr_m_jj, Cr_std_jj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_coord_NanNorm(piv_val, X_scaled, Y_scaled, r_bin_size):\n",
    "    'Calculate all possible distances within a FOV'\n",
    "    dists, r_bins = get_dists(X_scaled, Y_scaled, r_bin_size)\n",
    "\n",
    "    Cr_jj = []\n",
    "    N_jj = []\n",
    "\n",
    "    for jj in range(len(r_bins)):\n",
    "        if jj == 1:\n",
    "            r, s, t, u = np.where(dists == r_bins[jj])\n",
    "        else:\n",
    "            r, s, t, u = np.where((dists > r_bins[jj-1]) & (dists < r_bins[jj]))\n",
    "\n",
    "        Cr_kk = []\n",
    "        N_kk = []\n",
    "        for kk in range(len(piv_val)):\n",
    "            im = piv_val[kk]\n",
    "            im1 = []\n",
    "            im2 = []\n",
    "            for q in range(len(r)):\n",
    "                im_1 = im[r[q], s[q]]\n",
    "                im_2 = im[t[q], u[q]]\n",
    "                im1.append(im_1)\n",
    "                im2.append(im_2)\n",
    "\n",
    "            # makes sure the terms have same number of unmasked values\n",
    "            mask = np.divide(np.multiply(im1, im2), np.multiply(im1, im2))\n",
    "            im1 = np.multiply(mask, im1)\n",
    "            im2 = np.multiply(mask, im2)\n",
    "\n",
    "            corr_temp = np.multiply(im1, im2)\n",
    "            term1 = np.nansum(np.square(im1))\n",
    "            term2 = np.nansum(np.square(im2))\n",
    "            Cr = np.divide(np.nansum(corr_temp), np.sqrt(np.multiply(term1, term2)))\n",
    "            N = sum(~np.isnan(corr_temp))\n",
    "\n",
    "            Cr_kk.append(Cr)\n",
    "            N_kk.append(N)\n",
    "\n",
    "        Cr_jj.append(Cr_kk)\n",
    "        N_jj.append(N_kk)\n",
    "    \n",
    "    # get mean Cr for plotting functions\n",
    "    Cr_mean = np.nanmean(Cr_jj, axis=1)\n",
    "    \n",
    "    return(Cr_mean, r_bins, Cr_jj, N_jj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_coord_vector_NaNnorm(U_scaled, V_scaled, X_scaled, Y_scaled, r_bin_size):\n",
    "    'Calculate all possible distances within a FOV'\n",
    "    dists, r_bins = get_dists(X_scaled, Y_scaled, r_bin_size)\n",
    "\n",
    "    Cr_jj = []\n",
    "    N_jj = []\n",
    "    Cr_spatial_jj={}\n",
    "    Cr_m_jj = []\n",
    "    Cr_std_jj = []\n",
    "\n",
    "    for jj in range(len(r_bins)):\n",
    "        if jj == 1:\n",
    "            r, s, t, w = np.where(dists == r_bins[jj])\n",
    "        else:\n",
    "            r, s, t, w = np.where((dists > r_bins[jj-1]) & (dists < r_bins[jj]))\n",
    "\n",
    "        Cr_kk = []\n",
    "        N_kk = []\n",
    "        for kk in range(len(U_scaled)):\n",
    "            U_im = U_scaled[kk]\n",
    "            V_im = V_scaled[kk]\n",
    "\n",
    "            U_im1 = []\n",
    "            U_im2 = []\n",
    "            V_im1 = []\n",
    "            V_im2 = []\n",
    "            for q in range(len(r)):\n",
    "                U_im_1 = im[r[q], s[q]]\n",
    "                U_im_2 = im[t[q], u[q]]\n",
    "                V_im_1 = im[r[q], s[q]]\n",
    "                V_im_2 = im[t[q], u[q]]\n",
    "                U_im1.append(U_im_1)\n",
    "                U_im2.append(U_im_2)\n",
    "                V_im1.append(V_im_1)\n",
    "                V_im2.append(V_im_2)\n",
    "\n",
    "            # makes sure the terms have same number of ~unmasked values\n",
    "            mask = np.multiply(np.divide(U_im1, U_im1), np.divide(U_im2, U_im2), \n",
    "                               np.divide(V_im1, V_im1), np.divide(V_im2, V_im2))\n",
    "            U_im1 = np.multiply(mask, U_im1)\n",
    "            U_im2 = np.multiply(mask, U_im2)\n",
    "            V_im1 = np.multiply(mask, V_im1)\n",
    "            V_im2 = np.multiply(mask, V_im2)\n",
    "\n",
    "            u_corr_temp = np.multiply(U_im1, U_im2)    \n",
    "            v_corr_temp = np.multiply(V_im1, V_im2)    \n",
    "            corr_temp = u_corr_temp + v_corr_temp\n",
    "\n",
    "            u_term1 = np.nansum(np.square(U_im1))\n",
    "            u_term2 = np.nansum(np.square(U_im2))\n",
    "            v_term1 = np.nansum(np.square(V_im1))\n",
    "            v_term2 = np.nansum(np.square(V_im2))\n",
    "            term1 = u_term1 + v_term1\n",
    "            term2 = u_term2 + v_term2\n",
    "\n",
    "            Cr = np.divide(np.nansum(corr_temp), np.sqrt(np.multiply(term1, term2)))\n",
    "            N = sum(sum(~np.isnan(corr_temp)))\n",
    "\n",
    "            Cr_kk.append(Cr)\n",
    "            N_kk.append(N)\n",
    "        Cr_jj.append(Cr_kk)\n",
    "        N_jj.append(N_kk)\n",
    "        \n",
    "    return(Cr_jj, N_jj, r_bins)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piezo1-cKO PIV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pixelsize\n",
    "pixel_size= 0.6442*4\n",
    "# Set the timestep between frames\n",
    "delta_t = 5\n",
    "\n",
    "# Set which basefolder to cycle through for analysis\n",
    "folder=\"F:/Jesse/cKO/cKO_PIV/\"\n",
    "# folder = \"F:/Jesse/tdTomato+Drug/Yoda1_PIV/\"\n",
    "settings.save_path = folder\n",
    "images_list = sorted(glob(os.path.join(folder,\"Nuclei\", \"*.tif\")))\n",
    "binary_list =  sorted(glob(os.path.join(folder,\"Binary\", \"*.tif\")))\n",
    "\n",
    "# Doublecheck that our image list is matched to our binary list\n",
    "if len(images_list) != len(binary_list):\n",
    "    raise ValueError('Video length mismatch')\n",
    "\n",
    "angles_dict={}\n",
    "speed_dict = {}\n",
    "ang_dev_all={}\n",
    "\n",
    "rots=[]\n",
    "angles_rot_dict={}\n",
    "\n",
    "X_dict = {}\n",
    "Y_dict = {}\n",
    "U_dict = {}\n",
    "V_dict = {}\n",
    "v_rho_dict = {}\n",
    "v_theta_dict = {}\n",
    "\n",
    "# Create folder to save PIV results to\n",
    "settings.save_path = os.path.join(settings.save_path, \"Open_PIV_results\")\n",
    "if not os.path.exists(settings.save_path):\n",
    "    os.makedirs(settings.save_path)\n",
    "\n",
    "for i in tqdm(range(len(images_list)), position=1, leave=True):\n",
    "    \n",
    "    'General Housekeeping:'\n",
    "    # Grab the filename from filepath for later use\n",
    "    sfx = images_list[i].split('\\\\')\n",
    "    file_name = sfx[-1][:-4] \n",
    "    \n",
    "    nuclei_image = pims.TiffStack(images_list[i]) # Import image stack of nuclei_image[i]\n",
    "    binary_image = pims.TiffStack(binary_list[i]) # Import corresponding image mask[i]\n",
    "    \n",
    "    # Create empty arrays for OpenPIV fields to be appended to\n",
    "    U_raw=[]\n",
    "    V_raw=[]\n",
    "    X_raw=[]\n",
    "    Y_raw=[]\n",
    "    counter=0\n",
    "    \n",
    "    'Calculate multipass PIV Vector field for each frame pair in an image stack'\n",
    "    for image in tqdm(range(len(nuclei_image)-1), position=0, leave=True):\n",
    "        # Preprocessing of images\n",
    "        img_a = piv_pre.normalize_array(nuclei_image[image])\n",
    "        img_b = piv_pre.normalize_array(nuclei_image[image+1])\n",
    "        img_a_pre = exposure.equalize_adapthist(img_a, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)    \n",
    "        img_b_pre = exposure.equalize_adapthist(img_b, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)\n",
    "        img_a_pre = piv_pre.instensity_cap(img_a_pre, 4)\n",
    "        img_b_pre = piv_pre.instensity_cap(img_b_pre, 4)\n",
    "\n",
    "        # Mask the nuclei image using binarized image. White should denote regions to keep, Black is masked\n",
    "        settings.frame_pattern_a = cv.bitwise_and(cv.convertScaleAbs(img_a_pre),\n",
    "                                                  ~binary_image[image])\n",
    "        settings.frame_pattern_b = cv.bitwise_and(cv.convertScaleAbs(img_b_pre),\n",
    "                                                  ~binary_image[image+1])\n",
    "        mask_a = binary_image[image]\n",
    "        mask_b = binary_image[image+1]\n",
    "        x, y, u, v = piv(settings, file_name, mask_a, mask_b, counter)\n",
    "        counter=counter+1 #counter is for the image file save information\n",
    "        U_raw.append(u) #velocity vector in x direction\n",
    "        V_raw.append(v) #velocity vector in y direction\n",
    "        X_raw.append(x) #vector x possition\n",
    "        Y_raw.append(y) #vector y position\n",
    "    \n",
    "    'Scale X,Y,U,V information from pixels to microns'\n",
    "    X_scaled = np.array(X_raw)*pixel_size\n",
    "    Y_scaled = np.array(Y_raw)*pixel_size\n",
    "    U_scaled = np.array(U_raw)*(pixel_size/delta_t)\n",
    "    V_scaled = np.array(V_raw)*(pixel_size/delta_t)\n",
    "    \n",
    "    'Calculate speed of vectors'\n",
    "    speed_vec = np.square(U_scaled)+np.square(V_scaled)\n",
    "    speed_dict.update({file_name:speed_vec})\n",
    "\n",
    "    'Transform to Polar Coordinate'\n",
    "    rho = np.sqrt(np.square(X_scaled)+np.square(Y_scaled))\n",
    "    # v_rho or r_hat is the radial velocity\n",
    "    v_rho = np.divide((np.multiply(U_scaled, X_scaled) + np.multiply(V_scaled, Y_scaled)), rho)\n",
    "    # v_theta or theta_hat is the tangential velocity\n",
    "    v_theta = np.divide((np.multiply(-U_scaled, Y_scaled) + np.multiply(V_scaled, X_scaled)), rho)\n",
    "    # this is the angle of the piv vector\n",
    "    angles = np.arctan2(v_theta, v_rho)\n",
    "    \n",
    "    'Calculate Angular Deviation'\n",
    "    angle_dev_image=[]\n",
    "    ang=[]\n",
    "    for img in range(len(angles)):\n",
    "        # We will be ignoring any values==0 or nan as these would just be noise\n",
    "        ang.append(angles[img][~(angles[img]==0)])\n",
    "        ang[img] = ang[img][~np.isnan(ang[img])]\n",
    "        ang_dev = calc_ang_dev(ang[img])\n",
    "        angle_dev_image.append(ang_dev)\n",
    "    ang_dev_all.update({file_name:np.array(angle_dev_image)})\n",
    "    angles_dict.update({file_name:angles})\n",
    "    \n",
    "    'Find the rotation angle to account for variance in scratch angle'\n",
    "    rot_angle = straighten_scratch(binary_image)\n",
    "    rots.append(rot_angle)\n",
    "    X_rot, Y_rot = rotate_matrix(X_scaled, Y_scaled, rot_angle)\n",
    "    U_rot, V_rot = rotate_matrix(U_scaled, V_scaled, rot_angle)\n",
    "    \n",
    "    'Polar Coordinates of Rotated Matrix'\n",
    "    rho_rot = np.sqrt(np.square(X_rot)+np.square(Y_rot))\n",
    "    v_rho_rot = np.divide((np.multiply(U_rot, X_rot) + np.multiply(V_rot, Y_rot)), rho_rot)\n",
    "    v_theta_rot = np.divide((np.multiply(-U_rot, Y_rot) + np.multiply(V_rot, X_rot)), rho_rot)\n",
    "    angles_rot = np.arctan2(v_theta_rot, v_rho_rot)\n",
    "    angles_rot_dict.update({file_name:angles_rot})\n",
    "    \n",
    "    'Save X, Y, U, V, Rho_h, Theta_h info for later analysis'\n",
    "    X_dict.update({file_name:X_scaled})\n",
    "    Y_dict.update({file_name:Y_scaled})\n",
    "    U_dict.update({file_name:U_scaled})\n",
    "    V_dict.update({file_name:V_scaled})\n",
    "    v_rho_dict.update({file_name:v_rho})\n",
    "    v_theta_dict.update({file_name:v_theta})\n",
    "\n",
    "Direct_Var_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in ang_dev_all.items() ]))\n",
    "\n",
    "# Save dictionaries as pickle files\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(X_dict, builtins.open(f\"{date}Con&cKO_PIV_X_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(Y_dict, builtins.open(f\"{date}Con&cKO_PIV_Y_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(U_dict, builtins.open(f\"{date}Con&cKO_PIV_U_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(V_dict, builtins.open(f\"{date}Con&cKO_PIV_V_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_rho_dict, builtins.open(f\"{date}Con&cKO_PIV_vRho_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_theta_dict, builtins.open(f\"{date}Con&cKO_PIV_vTheta_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(ang_dev_all, builtins.open(f\"{date}Con&cKO_PIV_AngDev_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_dict, builtins.open(f\"{date}Con&cKO_PIV_Angles_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_rot_dict, builtins.open(f\"{date}Con&cKO_Angles_Rot_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(speed_dict, builtins.open(f\"{date}Con&cKO_PIV_Speed_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_vel_Cr_mean_dict={}\n",
    "rad_vel_r_bins_dict={}\n",
    "rad_vel_Cr_dict={}\n",
    "rad_vel_N_dict={}\n",
    "key_hole = list(X_dict.keys())\n",
    "for k in tqdm(range(len(key_hole)), position=0, leave=True):\n",
    "    rad_vel_Cr_mean, rad_vel_r_bins, rad_vel_Cr, rad_vel_N = spatial_coord_NanNorm(v_rho_dict[key_hole[k]], \n",
    "                                                                                    X_dict[key_hole[k]], \n",
    "                                                                                    Y_dict[key_hole[k]], 15)\n",
    "    rad_vel_Cr_mean_dict.update({key_hole[k]:rad_vel_Cr_mean})\n",
    "    rad_vel_r_bins_dict.update({key_hole[k]:rad_vel_r_bins})\n",
    "    rad_vel_Cr_dict.update({key_hole[k]:rad_vel_Cr})\n",
    "    rad_vel_N_dict.update({key_hole[k]:rad_vel_N})\n",
    "\n",
    "# Save dictionaries as pickle files\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(rad_vel_Cr_mean_dict, builtins.open(f\"{date}Con&cKO_PIV_RadVel_Cr_Mean_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_r_bins_dict, builtins.open(f\"{date}Con&cKO_PIV_RadVel_rbins_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_Cr_dict, builtins.open(f\"{date}Con&cKO_PIV_RadVel_Cr_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_N_dict, builtins.open(f\"{date}Con&cKO_PIV_RadVel_N_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pickle Files\n",
    "X_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_X_dict.p\", \"rb\"))\n",
    "Y_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_Y_dict.p\", \"rb\"))\n",
    "U_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_U_dict.p\", \"rb\"))\n",
    "V_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_V_dict.p\", \"rb\"))\n",
    "v_rho_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_vRho_dict.p\", \"rb\"))\n",
    "v_theta_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_vTheta_dict.p\", \"rb\"))\n",
    "ang_dev_all = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_AngDev_dict.p\", \"rb\"))\n",
    "angles_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_Angles_dict.p\", \"rb\"))\n",
    "angles_rot_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_Angles_Rot_dict.p\", \"rb\"))\n",
    "speed_dict = pickle.load(builtins.open(\"2022_05_12_Con&cKO_PIV_Speed_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_mean_dict = pickle.load(builtins.open(\"2022_05_15_Con&cKO_PIV_RadVel_Cr_Mean_dict.p\", \"rb\"))\n",
    "rad_vel_r_bins_dict = pickle.load(builtins.open(\"2022_05_15_Con&cKO_PIV_RadVel_rbins_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_dict = pickle.load(builtins.open(\"2022_05_15_Con&cKO_PIV_RadVel_Cr_dict.p\", \"rb\"))\n",
    "rad_vel_N_dict = pickle.load(builtins.open(\"2022_05_15_Con&cKO_PIV_RadVel_N_dict.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of each mean Cr for a FOV\n",
    "\n",
    "Con_cKO_Cr_mean=[]\n",
    "cKO_Cr_mean=[]\n",
    "\n",
    "key_hole = list(rad_vel_Cr_mean_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='cKO' or key_hole[k].split('_')[-4]=='cKO' or key_hole[k].split('_')[-3]=='cKO':\n",
    "        cKO_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    elif key_hole[k].split('_')[-2]=='Con' or key_hole[k].split('_')[-4]=='Con' or key_hole[k].split('_')[-3]=='Con':\n",
    "        Con_cKO_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "\n",
    "Con_Cr_avg = pd.DataFrame(Con_cKO_Cr_mean).T.mean(axis=1)\n",
    "Con_Cr_sem = pd.DataFrame(Con_cKO_Cr_mean).T.sem(axis=1)\n",
    "cKO_Cr_avg = pd.DataFrame(cKO_Cr_mean).T.mean(axis=1)\n",
    "cKO_Cr_sem = pd.DataFrame(cKO_Cr_mean).T.sem(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 114\n",
    "max_key = \"\"\n",
    "for key in rad_vel_r_bins_dict:\n",
    "    cur_len = len(rad_vel_r_bins_dict[key])\n",
    "    if cur_len==max_len:\n",
    "        max_key = key\n",
    "        max_len = cur_len\n",
    "print(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Local Coordination for Con/cKO\n",
    "col_names = rad_vel_r_bins_dict['317_2021_03_27_batch66_Pos8_cKO_top_prediction']\n",
    "df_1 = pd.DataFrame(cKO_Cr_mean)\n",
    "df_1.columns = col_names\n",
    "cKO_Cr_150 = df_1[150.0]\n",
    "\n",
    "col_names = rad_vel_r_bins_dict['317_2021_03_27_batch66_Pos40_Con_bot_prediction']\n",
    "df_2 = pd.DataFrame(Con_cKO_Cr_mean)\n",
    "df_2.columns = col_names\n",
    "Con_cKO_Cr_150 = df_2[150.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cKO_RadVel_All = pd.concat([pd.DataFrame({'R Bins':rad_vel_r_bins_dict['312_03_25_batch66_Pos10_Con_bot']}), \n",
    "           pd.DataFrame({'Con Radial Cr Mean':Con_Cr_avg}), \n",
    "           pd.DataFrame({'Con Radial Cr SEM':Con_Cr_sem}),\n",
    "           pd.DataFrame({'cKO Radial Cr Mean':cKO_Cr_avg}),\n",
    "           pd.DataFrame({'cKO Radial Cr SEM':cKO_Cr_sem})], axis=1)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "cKO_RadVel_All.to_excel(f'{date}_cKO_mean_Radial_Cr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cKO_Cr_fig = go.Figure()\n",
    "\n",
    "cKO_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['312_03_25_batch66_Pos10_Con_bot'][0:35], y= pd.DataFrame(Con_cKO_Cr_mean).T.mean(axis=1), name='Con(cKO)', line_color='#000000', error_y=dict(type='data', array = pd.DataFrame(Con_cKO_Cr_mean).T.sem(axis=1), visible=True)))\n",
    "cKO_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['312_03_25_batch66_Pos10_Con_bot'][0:35], y= pd.DataFrame(cKO_Cr_mean).T.mean(axis=1), name='cKO', line_color='#6a0dad', error_y =dict(type='data', array = pd.DataFrame(cKO_Cr_mean).T.sem(axis=1) , visible=True)))\n",
    "cKO_Cr_fig.update_layout(title=\"Spatial Coordination (Autocorrelation of Radial Velocity)\", xaxis_title=\"Δr(µm)\", yaxis_title=\"C(Δr)\", legend_title=\"Condition\")\n",
    "cKO_Cr_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cKO_Cr_mean_anova = []\n",
    "for col in range(6, np.shape(pd.DataFrame(Con_cKO_Cr_mean))[1]):\n",
    "    stat, p = stats.f_oneway(pd.DataFrame(Con_cKO_Cr_mean)[col], pd.DataFrame(cKO_Cr_mean)[col])\n",
    "    ks_stat, ks_p = stats.ks_2samp(pd.DataFrame(Con_cKO_Cr_mean)[col], pd.DataFrame(cKO_Cr_mean)[col])\n",
    "#     mw_stat, mw_p = stats.mannwhitneyu(pd.DataFrame(Con_cKO_Cr_mean)[col], pd.DataFrame(cKO_Cr_mean)[col])\n",
    "    d = cohend(pd.DataFrame(Con_cKO_Cr_mean)[col], pd.DataFrame(cKO_Cr_mean)[col])\n",
    "    cKO_Cr_mean_anova.append([stat, p, ks_stat, ks_p, mw_stat, mw_p, d])\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "cKO_Con_mean_stats = pd.DataFrame(cKO_Cr_mean_anova, columns= ['ANOVA stat', 'ANOVA p','KS Stat',' KS p','MW Stat',' MW p', 'Cohens d'])\n",
    "cKO_Con_mean_stats.to_excel(f'{date}_cKOvCon_Cr_mean_Stats.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_Cr_mean_dict={}\n",
    "angles_r_bins_dict={}\n",
    "angles_Cr_dict={}\n",
    "angles_N_dict={}\n",
    "key_hole = list(X_dict.keys())\n",
    "for k in tqdm(range(len(key_hole)), position=0, leave=True):\n",
    "    angles_Cr_mean, angles_r_bins, angles_Cr, angles_N = spatial_coord_NanNorm(v_theta_dict[key_hole[k]], \n",
    "                                        X_dict[key_hole[k]], \n",
    "                                        Y_dict[key_hole[k]], 15)\n",
    "    angles_Cr_mean_dict.update({key_hole[k]:tan_vel_Cr_mean})\n",
    "    angles_r_bins_dict.update({key_hole[k]:tan_vel_r_bins})\n",
    "    angles_Cr_dict.update({key_hole[k]:tan_vel_Cr})\n",
    "    angles_N_dict.update({key_hole[k]:tan_vel_N})\n",
    "\n",
    "# Save dictionaries as pickle files\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(angles_Cr_mean_dict, builtins.open(f\"{date}Con&cKO_PIV_Angles_Cr_Mean_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_r_bins_dict, builtins.open(f\"{date}Con&cKO_PIV_Angles_rbins_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_Cr_dict, builtins.open(f\"{date}Con&cKO_PIV_Angles_Cr_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_N_dict, builtins.open(f\"{date}Con&cKO_PIV_Angles_N_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_Cr_mean_dict = pickle.load(builtins.open(\"2022_05_19_Con&cKO_PIV_Angles_Cr_Mean_dict.p\", \"rb\"))\n",
    "angles_r_bins_dict = pickle.load(builtins.open(\"2022_05_19_Con&cKO_PIV_Angles_rbins_dict.p\", \"rb\"))\n",
    "angles_Cr_dict = pickle.load(builtins.open(\"2022_05_19_Con&cKO_PIV_Angles_Cr_dict.p\", \"rb\"))\n",
    "angles_N_dict = pickle.load(builtins.open(\"2022_05_19_Con&cKO_PIV_Angles_N_dict.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of each mean Cr for a FOV\n",
    "\n",
    "Con_cKO_angles_Cr_mean = []\n",
    "cKO_angles_Cr_mean = []\n",
    "\n",
    "key_hole = list(angles_Cr_mean_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='cKO' or key_hole[k].split('_')[-4]=='cKO' or key_hole[k].split('_')[-3]=='cKO':\n",
    "        cKO_angles_Cr_mean.append(angles_Cr_mean_dict[key_hole[k]])\n",
    "    elif key_hole[k].split('_')[-2]=='Con' or key_hole[k].split('_')[-4]=='Con' or key_hole[k].split('_')[-3]=='Con':\n",
    "        Con_cKO_angles_Cr_mean.append(angles_Cr_mean_dict[key_hole[k]])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "\n",
    "Angles_Con_Cr_avg = pd.DataFrame(Con_cKO_angles_Cr_mean).T.mean(axis=1)\n",
    "Angles_Con_Cr_sem = pd.DataFrame(Con_cKO_angles_Cr_mean).T.sem(axis=1)\n",
    "Angles_cKO_Cr_avg = pd.DataFrame(cKO_angles_Cr_mean).T.mean(axis=1)\n",
    "Angles_cKO_Cr_sem = pd.DataFrame(cKO_angles_Cr_mean).T.sem(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_cKO=[]\n",
    "cKO=[]\n",
    "speeds=[]\n",
    "key_hole = list(speed_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    speeds.append(np.array(speed_dict[key_hole[k]])[~np.isnan(speed_dict[key_hole[k]])])\n",
    "    speeds[k] = speeds[k][~(speeds[k]==0)]\n",
    "    if key_hole[k].split('_')[-2]=='cKO' or key_hole[k].split('_')[-4]=='cKO' or key_hole[k].split('_')[-3]=='cKO':\n",
    "        cKO.append(speeds[k])\n",
    "    elif key_hole[k].split('_')[-2]=='Con' or key_hole[k].split('_')[-4]=='Con' or key_hole[k].split('_')[-3]=='Con':\n",
    "        Con_cKO.append(speeds[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "cKO_speeds = np.concatenate(cKO)\n",
    "Con_cKO_speeds = np.concatenate(Con_cKO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_cKO=[]\n",
    "Con_names=[]\n",
    "cKO=[]\n",
    "cKO_names=[]\n",
    "\n",
    "ang=[]\n",
    "key_hole = list(angles_rot_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    ang.append(np.array(angles_rot_dict[key_hole[k]])[~np.isnan(angles_rot_dict[key_hole[k]])])\n",
    "    ang[k] = ang[k][~(ang[k]==0)]\n",
    "    ang[k] = ang[k]+((3*np.pi)/4)\n",
    "    for i in range(len(ang[k])):\n",
    "        if ang[k][i] >  np.pi:\n",
    "            ang[k][i] = ang[k][i]-(2*np.pi)\n",
    "        elif ang[k][i] <  -np.pi:\n",
    "            ang[k][i] = ang[k][i]+(2*np.pi)\n",
    "    if key_hole[k].split('_')[-2]=='cKO' or key_hole[k].split('_')[-4]=='cKO' or key_hole[k].split('_')[-3]=='cKO':\n",
    "        cKO.append(ang[k])\n",
    "        cKO_names.append(key_hole[k])\n",
    "    elif key_hole[k].split('_')[-2]=='Con' or key_hole[k].split('_')[-4]=='Con' or key_hole[k].split('_')[-3]=='Con':\n",
    "        Con_cKO.append(ang[k])\n",
    "        Con_names.append(key_hole[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "cKO_angles = np.concatenate(cKO)\n",
    "Con_cKO_angles = np.concatenate(Con_cKO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "Con_cKO_angles_df = pd.DataFrame(Con_cKO_angles)\n",
    "cKO_angles_df = pd.DataFrame(cKO_angles)\n",
    "Con_cKO_angles_df.to_csv(f'{date}_ConcKO_angles.csv')\n",
    "cKO_angles_df.to_csv(f'{date}_cKO_angles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cKO_angles = pd.read_csv (\"C:/Users/17605/Downloads/2022_04_30_02_06_PM_cKO_angles.csv\")\n",
    "Con_cKO_angles = pd.read_csv (\"C:/Users/17605/Downloads/2022_04_30_02_06_PM_ConcKO_angles.csv\")\n",
    "Con_cKO_angles.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "cKO_angles.drop(['Unnamed: 0'], axis=1,inplace=True)\n",
    "cKO_angles.rename({\"0\":\"Vector Direction (Degrees)\"}, axis=1, inplace=True)\n",
    "Con_cKO_angles.rename({\"0\":\"Vector Direction (Degrees)\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_cKO = pd.DataFrame(np.degrees(np.array(Con_cKO_angles)), columns=['Direction'])\n",
    "cKO = pd.DataFrame(np.degrees(np.array(cKO_angles)), columns=['Direction'])\n",
    "Con_cKO[\"Condition\"]=\"Con(cKO)\"\n",
    "cKO[\"Condition\"]=\"cKO\"\n",
    "cKO_v_Con_df = pd.concat([Con_cKO, cKO], axis=0)\n",
    "cKO_v_Con_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(cKO_v_Con_df, x=\"Direction\",\n",
    "             color=\"Condition\",\n",
    "             labels={\"Direction\":\"Vector Direction (Degrees)\"},\n",
    "             color_discrete_map = {'Control(cKO)':'#BAB0AC', 'cKO':'#A777F1'},\n",
    "             barmode=\"overlay\", \n",
    "             nbins=30,\n",
    "             range_x=[-180,180],\n",
    "             range_y=[0,15],\n",
    "             histnorm= \"percent\",\n",
    "             marginal=\"box\")\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_ConcCKO_Overlay_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(Con_cKO_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'Con Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['grey'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_Con_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(cKO_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'cKO Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['purple'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "fig.write_html(f'{date}_cKO_directionality_Figure.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_cKO_dev=[]\n",
    "cKO_dev=[]\n",
    "ConcKO_names=[]\n",
    "cKO_names=[]\n",
    "\n",
    "key_hole = list(ang_dev_all.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-3]=='Con' or key_hole[k].split('_')[-4]=='Con'  or key_hole[k].split('_')[-2]=='Con':\n",
    "        Con_cKO_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        ConcKO_names.append(key_hole[k])\n",
    "\n",
    "    elif key_hole[k].split('_')[-3]=='cKO' or key_hole[k].split('_')[-4]=='cKO' or  key_hole[k].split('_')[-2]=='cKO':\n",
    "        cKO_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        cKO_names.append(key_hole[k])\n",
    "\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_v_cKO_angdev = pd.concat([pd.DataFrame(ConcKO_names, columns=['Filename']),\n",
    "                        pd.DataFrame(Con_cKO_dev, columns=['Control(cKO)']),\n",
    "                        pd.DataFrame(cKO_names, columns=['Filename']),\n",
    "                        pd.DataFrame(cKO_dev, columns=['cKO'])], axis=1)\n",
    "con_v_cKO_angdev.to_excel(f'{date}cKO_v_Con_AngDev.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cko_ang_dev = pd.concat([pd.DataFrame(Con_cKO_dev, columns=['Control(cKO)']),\n",
    "                      pd.DataFrame(cKO_dev, columns=['cKO'])], axis=1)\n",
    "cKO_ang_dev_dabest = dabest.load(cko_ang_dev, idx=(\"Control(cKO)\", \"cKO\"), resamples=5000)\n",
    "cKO_ang_dev_dabest.cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pal={'Control(cKO)':'black', 'cKO': 'purple'}\n",
    "cKO_ang_dev_dabest.cohens_d.plot(swarm_label='Angular Deviation',float_contrast=False, custom_palette=color_pal, raw_marker_size=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoda1 PIV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pixelsize\n",
    "pixel_size= 0.6442*4\n",
    "# Set the timestep between frames\n",
    "delta_t = 5\n",
    "\n",
    "# Set which basefolder to cycle through for analysis\n",
    "\n",
    "folder = \"F:/Jesse/tdTomato+Drug/Yoda1_PIV/\"\n",
    "settings.save_path = folder\n",
    "images_list = sorted(glob(os.path.join(folder,\"Nuclei\", \"*.tif\")))\n",
    "binary_list =  sorted(glob(os.path.join(folder,\"Binary\", \"*.tif\")))\n",
    "\n",
    "# Doublecheck that our image list is matched to our binary list\n",
    "if len(images_list) != len(binary_list):\n",
    "    raise ValueError('Video length mismatch')\n",
    "\n",
    "angles_dict={}\n",
    "speed_dict = {}\n",
    "ang_dev_all={}\n",
    "\n",
    "rots=[]\n",
    "angles_rot_dict={}\n",
    "\n",
    "X_dict = {}\n",
    "Y_dict = {}\n",
    "U_dict = {}\n",
    "V_dict = {}\n",
    "v_rho_dict = {}\n",
    "v_theta_dict = {}\n",
    "\n",
    "# Create folder to save PIV results to\n",
    "settings.save_path = os.path.join(settings.save_path, \"Open_PIV_results\")\n",
    "if not os.path.exists(settings.save_path):\n",
    "    os.makedirs(settings.save_path)\n",
    "\n",
    "for i in tqdm(range(len(images_list)), position=1, leave=True):\n",
    "    \n",
    "    'General Housekeeping:'\n",
    "    # Grab the filename from filepath for later use\n",
    "    sfx = images_list[i].split('\\\\')\n",
    "    file_name = sfx[-1][:-4] \n",
    "    \n",
    "    nuclei_image = pims.TiffStack(images_list[i]) # Import image stack of nuclei_image[i]\n",
    "    binary_image = pims.TiffStack(binary_list[i]) # Import corresponding image mask[i]\n",
    "    \n",
    "    # Create empty arrays for OpenPIV fields to be appended to\n",
    "    U_raw=[]\n",
    "    V_raw=[]\n",
    "    X_raw=[]\n",
    "    Y_raw=[]\n",
    "    counter=0\n",
    "    \n",
    "    'Calculate multipass PIV Vector field for each frame pair in an image stack'\n",
    "    for image in tqdm(range(len(nuclei_image)-1), position=0, leave=True):\n",
    "        # Preprocessing of images\n",
    "        img_a = piv_pre.normalize_array(nuclei_image[image])\n",
    "        img_b = piv_pre.normalize_array(nuclei_image[image+1])\n",
    "        img_a_pre = exposure.equalize_adapthist(img_a, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)    \n",
    "        img_b_pre = exposure.equalize_adapthist(img_b, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)\n",
    "        img_a_pre = piv_pre.instensity_cap(img_a_pre, 4)\n",
    "        img_b_pre = piv_pre.instensity_cap(img_b_pre, 4)\n",
    "\n",
    "        # Mask the nuclei image using binarized image. White should denote regions to keep, Black is masked\n",
    "        settings.frame_pattern_a = cv.bitwise_and(cv.convertScaleAbs(img_a_pre),\n",
    "                                                  ~binary_image[image])\n",
    "        settings.frame_pattern_b = cv.bitwise_and(cv.convertScaleAbs(img_b_pre),\n",
    "                                                  ~binary_image[image+1])\n",
    "        mask_a = binary_image[image]\n",
    "        mask_b = binary_image[image+1]\n",
    "        x, y, u, v = piv(settings, file_name, mask_a, mask_b, counter)\n",
    "        counter=counter+1 #counter is for the image file save information\n",
    "        U_raw.append(u) #velocity vector in x direction\n",
    "        V_raw.append(v) #velocity vector in y direction\n",
    "        X_raw.append(x) #vector x possition\n",
    "        Y_raw.append(y) #vector y position\n",
    "    \n",
    "    'Scale X,Y,U,V information from pixels to microns'\n",
    "    X_scaled = np.array(X_raw)*pixel_size\n",
    "    Y_scaled = np.array(Y_raw)*pixel_size\n",
    "    U_scaled = np.array(U_raw)*(pixel_size/delta_t)\n",
    "    V_scaled = np.array(V_raw)*(pixel_size/delta_t)\n",
    "    \n",
    "    'Calculate speed of vectors'\n",
    "    speed_vec = np.square(U_scaled)+np.square(V_scaled)\n",
    "    speed_dict.update({file_name:speed_vec})\n",
    "\n",
    "    'Transform to Polar Coordinate'\n",
    "    rho = np.sqrt(np.square(X_scaled)+np.square(Y_scaled))\n",
    "    # v_rho or r_hat is the radial velocity\n",
    "    v_rho = np.divide((np.multiply(U_scaled, X_scaled) + np.multiply(V_scaled, Y_scaled)), rho)\n",
    "    # v_theta or theta_hat is the tangential velocity\n",
    "    v_theta = np.divide((np.multiply(-U_scaled, Y_scaled) + np.multiply(V_scaled, X_scaled)), rho)\n",
    "    # this is the angle of the piv vector\n",
    "    angles = np.arctan2(v_theta, v_rho)\n",
    "    \n",
    "    'Calculate Angular Deviation'\n",
    "    angle_dev_image=[]\n",
    "    ang=[]\n",
    "    for img in range(len(angles)):\n",
    "        # We will be ignoring any values==0 or nan as these would just be noise\n",
    "        ang.append(angles[img][~(angles[img]==0)])\n",
    "        ang[img] = ang[img][~np.isnan(ang[img])]\n",
    "        ang_dev = calc_ang_dev(ang[img])\n",
    "        angle_dev_image.append(ang_dev)\n",
    "    ang_dev_all.update({file_name:np.array(angle_dev_image)})\n",
    "    angles_dict.update({file_name:angles})\n",
    "    \n",
    "    'Find the rotation angle to account for variance in scratch angle'\n",
    "    rot_angle = straighten_scratch(binary_image)\n",
    "    rots.append(rot_angle)\n",
    "    X_rot, Y_rot = rotate_matrix(X_scaled, Y_scaled, rot_angle)\n",
    "    U_rot, V_rot = rotate_matrix(U_scaled, V_scaled, rot_angle)\n",
    "    \n",
    "    'Polar Coordinates of Rotated Matrix'\n",
    "    rho_rot = np.sqrt(np.square(X_rot)+np.square(Y_rot))\n",
    "    v_rho_rot = np.divide((np.multiply(U_rot, X_rot) + np.multiply(V_rot, Y_rot)), rho_rot)\n",
    "    v_theta_rot = np.divide((np.multiply(-U_rot, Y_rot) + np.multiply(V_rot, X_rot)), rho_rot)\n",
    "    angles_rot = np.arctan2(v_theta_rot, v_rho_rot)\n",
    "    angles_rot_dict.update({file_name:angles_rot})\n",
    "    \n",
    "    'Save X, Y, U, V, Rho_h, Theta_h info for later analysis'\n",
    "    X_dict.update({file_name:X_scaled})\n",
    "    Y_dict.update({file_name:Y_scaled})\n",
    "    U_dict.update({file_name:U_scaled})\n",
    "    V_dict.update({file_name:V_scaled})\n",
    "    v_rho_dict.update({file_name:v_rho})\n",
    "    v_theta_dict.update({file_name:v_theta})\n",
    "\n",
    "Direct_Var_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in ang_dev_all.items() ]))\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(X_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_X_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(Y_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_Y_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(U_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_U_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(V_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_V_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_rho_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_vRho_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_theta_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_vTheta_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(ang_dev_all, builtins.open(f\"{date}DMSO&Yoda1_PIV_AngDev_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_Angles_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_rot_dict, builtins.open(f\"{date}DMSO&Yoda1_Angles_Rot_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(speed_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_Speed_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pickle Files\n",
    "X_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_X_dict.p\", \"rb\"))\n",
    "Y_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_Y_dict.p\", \"rb\"))\n",
    "U_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_U_dict.p\", \"rb\"))\n",
    "V_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_V_dict.p\", \"rb\"))\n",
    "v_rho_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_vRho_dict.p\", \"rb\"))\n",
    "v_theta_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_vTheta_dict.p\", \"rb\"))\n",
    "ang_dev_all = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_AngDev_dict.p\", \"rb\"))\n",
    "angles_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_Angles_dict.p\", \"rb\"))\n",
    "angles_rot_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_Angles_Rot_dict.p\", \"rb\"))\n",
    "speed_dict = pickle.load(builtins.open(\"2022_06_21_DMSO&Yoda1_PIV_Speed_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_mean_dict = pickle.load(builtins.open(\"2022_06_22_DMSO&Yoda1_PIV_RadVel_Cr_Mean_dict.p\", \"rb\"))\n",
    "rad_vel_r_bins_dict = pickle.load(builtins.open(\"2022_06_22_DMSO&Yoda1_PIV_RadVel_rbins_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_dict = pickle.load(builtins.open(\"2022_06_22_DMSO&Yoda1_PIV_RadVel_Cr_dict.p\", \"rb\"))\n",
    "rad_vel_N_dict = pickle.load(builtins.open(\"2022_06_22_DMSO&Yoda1_PIV_RadVel_N_dict.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spatial Autocorrelation of Radial Velocity\n",
    "rad_vel_Cr_mean_dict={}\n",
    "rad_vel_r_bins_dict={}\n",
    "rad_vel_Cr_dict={}\n",
    "rad_vel_N_dict={}\n",
    "key_hole = list(X_dict.keys())\n",
    "for k in tqdm(range(len(key_hole)), position=0, leave=True):\n",
    "    rad_vel_Cr_mean, rad_vel_r_bins, rad_vel_Cr, rad_vel_N = spatial_coord_NanNorm(v_rho_dict[key_hole[k]], \n",
    "                                                                                    X_dict[key_hole[k]], \n",
    "                                                                                    Y_dict[key_hole[k]], 15)\n",
    "    rad_vel_Cr_mean_dict.update({key_hole[k]:rad_vel_Cr_mean})\n",
    "    rad_vel_r_bins_dict.update({key_hole[k]:rad_vel_r_bins})\n",
    "    rad_vel_Cr_dict.update({key_hole[k]:rad_vel_Cr})\n",
    "    rad_vel_N_dict.update({key_hole[k]:rad_vel_N})\n",
    "\n",
    "# Save dictionaries as pickle files\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(rad_vel_Cr_mean_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_RadVel_Cr_Mean_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_r_bins_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_RadVel_rbins_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_Cr_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_RadVel_Cr_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_N_dict, builtins.open(f\"{date}DMSO&Yoda1_PIV_RadVel_N_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Spatial Coordination and SEM for Plotting\n",
    "\n",
    "DMSO_Cr_mean = []\n",
    "Yoda1_Cr_mean = []\n",
    "\n",
    "key_hole = list(rad_vel_Cr_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='DMSO' or key_hole[k].split('_')[-3]=='DMSO':\n",
    "        DMSO_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    elif key_hole[k].split('_')[-2]=='Yoda1' or key_hole[k].split('_')[-2]=='Y1' or key_hole[k].split('_')[-3]=='Yoda1' or key_hole[k].split('_')[-4]=='Yoda1':\n",
    "        Yoda1_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "\n",
    "DMSO_Cr_avg = pd.DataFrame(DMSO_Cr_mean).T.mean(axis=1)\n",
    "DMSO_Cr_sem = pd.DataFrame(DMSO_Cr_mean).T.sem(axis=1)\n",
    "Yoda1_Cr_avg = pd.DataFrame(Yoda1_Cr_mean).T.mean(axis=1)\n",
    "Yoda1_Cr_sem = pd.DataFrame(Yoda1_Cr_mean).T.sem(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 112\n",
    "max_key = \"\"\n",
    "for key in rad_vel_r_bins_dict:\n",
    "    cur_len = len(rad_vel_r_bins_dict[key])\n",
    "    if cur_len==max_len:\n",
    "        max_key = key\n",
    "        max_len = cur_len\n",
    "print(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Local Coordination for DMSO/Yoda1\n",
    "col_names = rad_vel_r_bins_dict['263_2020_08_20_Scratch_Pos7_Yoda1_top']\n",
    "df_1 = pd.DataFrame(Yoda1_Cr_mean)\n",
    "df_1.columns = col_names\n",
    "Yoda1_Cr_150 = df_1[150.0]\n",
    "\n",
    "col_names = rad_vel_r_bins_dict['332_2021_05_02_batch70_Pos20_skin3_Y1_top']\n",
    "df_2 = pd.DataFrame(DMSO_Cr_mean)\n",
    "df_2.columns = col_names\n",
    "DMSO_Cr_150 = df_2[150.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of Mean spatial autocorrelation of Radial Velocity component\n",
    "Yoda1_Cr_fig = go.Figure()\n",
    "\n",
    "Yoda1_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['332_2021_05_02_batch70_DMSO_Pos3_top'][0:35], y= pd.DataFrame(DMSO_Cr_mean).T.mean(axis=1), name='DMSO', line_color='#000000', error_y=dict(type='data', array = pd.DataFrame(DMSO_Cr_mean).T.sem(axis=1), visible=True)))\n",
    "Yoda1_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['332_2021_05_02_batch70_DMSO_Pos3_top'][0:35], y= pd.DataFrame(Yoda1_Cr_mean).T.mean(axis=1), name='Yoda1', line_color='#ff0000', error_y =dict(type='data', array = pd.DataFrame(Yoda1_Cr_mean).T.sem(axis=1) , visible=True)))\n",
    "Yoda1_Cr_fig.update_layout(title=\"Spatial Coordination (Autocorrelation of Radial Velocity)\", xaxis_title=\"Δr(µm)\", yaxis_title=\"C(Δr)\", legend_title=\"Condition\")\n",
    "Yoda1_Cr_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMSO_RadVel_All = pd.concat([pd.DataFrame({'R Bins':rad_vel_r_bins_dict['332_2021_05_02_batch70_DMSO_Pos3_top']}), \n",
    "           pd.DataFrame({'DMSO Radial Cr Mean':DMSO_Cr_avg}), \n",
    "           pd.DataFrame({'DMSO Radial Cr SEM':DMSO_Cr_sem}),\n",
    "           pd.DataFrame({'Yoda1 Radial Cr Mean':Yoda1_Cr_avg}),\n",
    "           pd.DataFrame({'Yoda1 Radial Cr SEM':Yoda1_Cr_sem})], axis=1)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "DMSO_RadVel_All.to_excel(f'{date}_DMSO&Yoda1_mean_Radial_Cr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yoda1_Cr_mean_anova = []\n",
    "for col in range(0,np.shape(pd.DataFrame(DMSO_Cr_mean))[1]):\n",
    "    stat, p = stats.f_oneway(pd.DataFrame(DMSO_Cr_mean)[col], pd.DataFrame(Yoda1_Cr_mean)[col])\n",
    "    ks_stat, ks_p = stats.ks_2samp(pd.DataFrame(DMSO_Cr_mean)[col], pd.DataFrame(Yoda1_Cr_mean)[col])\n",
    "    if len(pd.DataFrame(DMSO_Cr_mean)[col].dropna())>0 & len(pd.DataFrame(Yoda1_Cr_mean)[col].dropna())>0:\n",
    "        mw_stat, mw_p = stats.mannwhitneyu(pd.DataFrame(DMSO_Cr_mean)[col].dropna(), pd.DataFrame(Yoda1_Cr_mean)[col].dropna())\n",
    "    else:\n",
    "        mw_stat, mw_p = 0, 0\n",
    "    d = cohend(pd.DataFrame(DMSO_Cr_mean)[col], pd.DataFrame(Yoda1_Cr_mean)[col])\n",
    "    Yoda1_Cr_mean_anova.append([stat, p, ks_stat, ks_p, mw_stat, mw_p, d])\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "cKO_Con_mean_stats = pd.DataFrame(Yoda1_Cr_mean_anova, columns= ['ANOVA stat', 'ANOVA p','KS Stat',' KS p','MW Stat',' MW p', 'Cohens d'])\n",
    "cKO_Con_mean_stats.to_excel(f'{date}DMSO&Yoda1_Cr_mean_Stats.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMSO=[]\n",
    "DMSO_names=[]\n",
    "Yoda1=[]\n",
    "Yoda1_names=[]\n",
    "\n",
    "ang=[]\n",
    "key_hole = list(angles_rot_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    ang.append(np.array(angles_rot_dict[key_hole[k]])[~np.isnan(angles_rot_dict[key_hole[k]])])\n",
    "    ang[k] = ang[k][~(ang[k]==0)]\n",
    "    ang[k] = ang[k]+((3*np.pi)/4)\n",
    "    for i in range(len(ang[k])):\n",
    "        if ang[k][i] >  np.pi:\n",
    "            ang[k][i] = ang[k][i]-(2*np.pi)\n",
    "        elif ang[k][i] <  -np.pi:\n",
    "            ang[k][i] = ang[k][i]+(2*np.pi)\n",
    "    if key_hole[k].split('_')[-2]=='DMSO' or key_hole[k].split('_')[-3]=='DMSO':\n",
    "        DMSO.append(ang[k])\n",
    "        DMSO_names.append(key_hole[k])\n",
    "    elif key_hole[k].split('_')[-2]=='Yoda1' or key_hole[k].split('_')[-2]=='Y1' or key_hole[k].split('_')[-3]=='Yoda1' or key_hole[k].split('_')[-4]=='Yoda1':\n",
    "        Yoda1.append(ang[k])\n",
    "        Yoda1_names.append(key_hole[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "DMSO_angles = np.concatenate(DMSO)\n",
    "Yoda1_angles = np.concatenate(Yoda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "DMSO_angles_df = pd.DataFrame(DMSO_angles)\n",
    "Yoda1_angles_df = pd.DataFrame(Yoda1_angles)\n",
    "DMSO_angles_df.to_csv(f'{date}_DMSO_angles.csv')\n",
    "Yoda1_angles_df.to_csv(f'{date}_Yoda1_angles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMSO = pd.DataFrame(np.degrees(np.array(DMSO_angles)), columns=['Direction'])\n",
    "Yoda1 = pd.DataFrame(np.degrees(np.array(Yoda1_angles)), columns=['Direction'])\n",
    "DMSO[\"Condition\"]=\"DMSO\"\n",
    "Yoda1[\"Condition\"]=\"Yoda1\"\n",
    "DMSO_v_Yoda1_df = pd.concat([DMSO, Yoda1], axis=0)\n",
    "DMSO_v_Yoda1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Figures for Yoda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(DMSO_v_Yoda1_df, x=\"Direction\",\n",
    "             color=\"Condition\",\n",
    "             labels={\"Direction\":\"Vector Direction (Degrees)\"},\n",
    "             color_discrete_map = {'DMSO':'#BAB0AC', 'Yoda1':'#ff0000'},\n",
    "             barmode=\"overlay\", \n",
    "             nbins=30,\n",
    "             range_x=[-180,180],\n",
    "             range_y=[0,10],\n",
    "             histnorm= \"percent\",\n",
    "             marginal=\"box\")\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_DMSO&Yoda1_Overlay_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(DMSO_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'DMSO Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['grey'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_DMSO_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(Yoda1_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'Yoda1 Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['red'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "fig.write_html(f'{date}_Yoda1_directionality_Figure.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Angular Deviation\n",
    "Yoda1_dev=[]\n",
    "DMSO_dev=[]\n",
    "DMSO_names=[]\n",
    "Yoda1_names=[]\n",
    "\n",
    "key_hole = list(ang_dev_all.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='Yoda1' or key_hole[k].split('_')[-2]=='Y1' or key_hole[k].split('_')[-3]=='Yoda1' or key_hole[k].split('_')[-4]=='Yoda1':\n",
    "        Yoda1_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        Yoda1_names.append(key_hole[k])\n",
    "    elif key_hole[k].split('_')[-2]=='DMSO' or key_hole[k].split('_')[-3]=='DMSO':\n",
    "        DMSO_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        DMSO_names.append(key_hole[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMSO_v_Yoda1_angdev = pd.concat([pd.DataFrame(DMSO_names, columns=['Filename']),\n",
    "                        pd.DataFrame(DMSO_dev, columns=['DMSO']),\n",
    "                        pd.DataFrame(Yoda1_names, columns=['Filename']),\n",
    "                        pd.DataFrame(Yoda1_dev, columns=['Yoda1'])], axis=1)\n",
    "DMSO_v_Yoda1_angdev.to_excel(f'{date}DMSO_v_Yoda1_AngDev.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_ang_dev = pd.concat([pd.DataFrame(DMSO_dev, columns=['DMSO']),\n",
    "                      pd.DataFrame(Yoda1_dev, columns=['Yoda1'])], axis=1)\n",
    "y1_ang_dev_dabest = dabest.load(y1_ang_dev, idx=(\"DMSO\", \"Yoda1\"), resamples=5000)\n",
    "y1_ang_dev_dabest.cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y1_ang_dev_dabest.cohens_d.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pal={'DMSO':'black', 'Yoda1': 'red'}\n",
    "y1_ang_dev_dabest.cohens_d.plot(swarm_label='Angular Deviation',float_contrast=False, custom_palette=color_pal, raw_marker_size=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piezo1-Gof PIV Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the pixelsize\n",
    "pixel_size= 0.6442*4\n",
    "# Set the timestep between frames\n",
    "delta_t = 5\n",
    "\n",
    "# Set which basefolder to cycle through for analysis\n",
    "\n",
    "folder = \"F:/Jesse/GoF/GoF_PIV/\"\n",
    "settings.save_path = folder\n",
    "images_list = sorted(glob(os.path.join(folder,\"Nuclei\", \"*.tif\")))\n",
    "binary_list =  sorted(glob(os.path.join(folder,\"Binary\", \"*.tif\")))\n",
    "\n",
    "# Doublecheck that our image list is matched to our binary list\n",
    "if len(images_list) != len(binary_list):\n",
    "    raise ValueError('Video length mismatch')\n",
    "\n",
    "angles_dict={}\n",
    "speed_dict = {}\n",
    "ang_dev_all={}\n",
    "\n",
    "rots=[]\n",
    "angles_rot_dict={}\n",
    "\n",
    "X_dict = {}\n",
    "Y_dict = {}\n",
    "U_dict = {}\n",
    "V_dict = {}\n",
    "v_rho_dict = {}\n",
    "v_theta_dict = {}\n",
    "\n",
    "# Create folder to save PIV results to\n",
    "settings.save_path = os.path.join(settings.save_path, \"Open_PIV_results\")\n",
    "if not os.path.exists(settings.save_path):\n",
    "    os.makedirs(settings.save_path)\n",
    "\n",
    "for i in tqdm(range(len(images_list)), position=1, leave=True):\n",
    "    \n",
    "    'General Housekeeping:'\n",
    "    # Grab the filename from filepath for later use\n",
    "    sfx = images_list[i].split('\\\\')\n",
    "    file_name = sfx[-1][:-4] \n",
    "    \n",
    "    nuclei_image = pims.TiffStack(images_list[i]) # Import image stack of nuclei_image[i]\n",
    "    binary_image = pims.TiffStack(binary_list[i]) # Import corresponding image mask[i]\n",
    "    \n",
    "    # Create empty arrays for OpenPIV fields to be appended to\n",
    "    U_raw=[]\n",
    "    V_raw=[]\n",
    "    X_raw=[]\n",
    "    Y_raw=[]\n",
    "    counter=0\n",
    "    \n",
    "    'Calculate multipass PIV Vector field for each frame pair in an image stack'\n",
    "    for image in tqdm(range(len(nuclei_image)-1), position=0, leave=True):\n",
    "        # Preprocessing of images\n",
    "        img_a = piv_pre.normalize_array(nuclei_image[image])\n",
    "        img_b = piv_pre.normalize_array(nuclei_image[image+1])\n",
    "        img_a_pre = exposure.equalize_adapthist(img_a, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)    \n",
    "        img_b_pre = exposure.equalize_adapthist(img_b, \n",
    "                                               kernel_size = None, \n",
    "                                               clip_limit = 0.05, \n",
    "                                               nbins = 256)\n",
    "        img_a_pre = piv_pre.instensity_cap(img_a_pre, 4)\n",
    "        img_b_pre = piv_pre.instensity_cap(img_b_pre, 4)\n",
    "\n",
    "        # Mask the nuclei image using binarized image. White should denote regions to keep, Black is masked\n",
    "        settings.frame_pattern_a = cv.bitwise_and(cv.convertScaleAbs(img_a_pre),\n",
    "                                                  ~binary_image[image])\n",
    "        settings.frame_pattern_b = cv.bitwise_and(cv.convertScaleAbs(img_b_pre),\n",
    "                                                  ~binary_image[image+1])\n",
    "        mask_a = binary_image[image]\n",
    "        mask_b = binary_image[image+1]\n",
    "        x, y, u, v = piv(settings, file_name, mask_a, mask_b, counter)\n",
    "        counter=counter+1 #counter is for the image file save information\n",
    "        U_raw.append(u) #velocity vector in x direction\n",
    "        V_raw.append(v) #velocity vector in y direction\n",
    "        X_raw.append(x) #vector x possition\n",
    "        Y_raw.append(y) #vector y position\n",
    "    \n",
    "    'Scale X,Y,U,V information from pixels to microns'\n",
    "    X_scaled = np.array(X_raw)*pixel_size\n",
    "    Y_scaled = np.array(Y_raw)*pixel_size\n",
    "    U_scaled = np.array(U_raw)*(pixel_size/delta_t)\n",
    "    V_scaled = np.array(V_raw)*(pixel_size/delta_t)\n",
    "    \n",
    "    'Calculate speed of vectors'\n",
    "    speed_vec = np.square(U_scaled)+np.square(V_scaled)\n",
    "    speed_dict.update({file_name:speed_vec})\n",
    "\n",
    "    'Transform to Polar Coordinate'\n",
    "    rho = np.sqrt(np.square(X_scaled)+np.square(Y_scaled))\n",
    "    # v_rho or r_hat is the radial velocity\n",
    "    v_rho = np.divide((np.multiply(U_scaled, X_scaled) + np.multiply(V_scaled, Y_scaled)), rho)\n",
    "    # v_theta or theta_hat is the tangential velocity\n",
    "    v_theta = np.divide((np.multiply(-U_scaled, Y_scaled) + np.multiply(V_scaled, X_scaled)), rho)\n",
    "    # this is the angle of the piv vector\n",
    "    angles = np.arctan2(v_theta, v_rho)\n",
    "    \n",
    "    'Calculate Angular Deviation'\n",
    "    angle_dev_image=[]\n",
    "    ang=[]\n",
    "    for img in range(len(angles)):\n",
    "        # We will be ignoring any values==0 or nan as these would just be noise\n",
    "        ang.append(angles[img][~(angles[img]==0)])\n",
    "        ang[img] = ang[img][~np.isnan(ang[img])]\n",
    "        ang_dev = calc_ang_dev(ang[img])\n",
    "        angle_dev_image.append(ang_dev)\n",
    "    ang_dev_all.update({file_name:np.array(angle_dev_image)})\n",
    "    angles_dict.update({file_name:angles})\n",
    "    \n",
    "    'Find the rotation angle to account for variance in scratch angle'\n",
    "    rot_angle = straighten_scratch(binary_image)\n",
    "    rots.append(rot_angle)\n",
    "    X_rot, Y_rot = rotate_matrix(X_scaled, Y_scaled, rot_angle)\n",
    "    U_rot, V_rot = rotate_matrix(U_scaled, V_scaled, rot_angle)\n",
    "    \n",
    "    'Polar Coordinates of Rotated Matrix'\n",
    "    rho_rot = np.sqrt(np.square(X_rot)+np.square(Y_rot))\n",
    "    v_rho_rot = np.divide((np.multiply(U_rot, X_rot) + np.multiply(V_rot, Y_rot)), rho_rot)\n",
    "    v_theta_rot = np.divide((np.multiply(-U_rot, Y_rot) + np.multiply(V_rot, X_rot)), rho_rot)\n",
    "    angles_rot = np.arctan2(v_theta_rot, v_rho_rot)\n",
    "    angles_rot_dict.update({file_name:angles_rot})\n",
    "    \n",
    "    'Save X, Y, U, V, Rho_h, Theta_h info for later analysis'\n",
    "    X_dict.update({file_name:X_scaled})\n",
    "    Y_dict.update({file_name:Y_scaled})\n",
    "    U_dict.update({file_name:U_scaled})\n",
    "    V_dict.update({file_name:V_scaled})\n",
    "    v_rho_dict.update({file_name:v_rho})\n",
    "    v_theta_dict.update({file_name:v_theta})\n",
    "\n",
    "Direct_Var_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in ang_dev_all.items() ]))\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "pickle.dump(X_dict, builtins.open(f\"{date}Con&GoF_PIV_X_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(Y_dict, builtins.open(f\"{date}Con&GoF_PIV_Y_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(U_dict, builtins.open(f\"{date}Con&GoF_PIV_U_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(V_dict, builtins.open(f\"{date}Con&GoF_PIV_V_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_rho_dict, builtins.open(f\"{date}Con&GoF_PIV_vRho_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(v_theta_dict, builtins.open(f\"{date}Con&GoF_PIV_vTheta_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(ang_dev_all, builtins.open(f\"{date}Con&GoF_PIV_AngDev_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_dict, builtins.open(f\"{date}Con&GoF_PIV_Angles_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(angles_rot_dict, builtins.open(f\"{date}Con&GoF_Angles_Rot_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(speed_dict, builtins.open(f\"{date}Con&GoF_PIV_Speed_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Pass\n",
    "X_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_X_dict.p\", \"rb\"))\n",
    "Y_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_Y_dict.p\", \"rb\"))\n",
    "U_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_U_dict.p\", \"rb\"))\n",
    "V_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_V_dict.p\", \"rb\"))\n",
    "v_rho_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_vRho_dict.p\", \"rb\"))\n",
    "v_theta_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_vTheta_dict.p\", \"rb\"))\n",
    "ang_dev_all = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_AngDev_dict.p\", \"rb\"))\n",
    "angles_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_Angles_dict.p\", \"rb\"))\n",
    "angles_rot_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_Angles_Rot_dict.p\", \"rb\"))\n",
    "speed_dict = pickle.load(builtins.open(\"2022_06_22_Con&GoF_PIV_Speed_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_mean_dict = pickle.load(builtins.open(\"2022_06_23_04_01_PM_Con&GoF_PIV_RadVel_Cr_Mean_dict.p\", \"rb\"))\n",
    "rad_vel_r_bins_dict = pickle.load(builtins.open(\"2022_06_23_04_01_PM_Con&GoF_PIV_RadVel_rbins_dict.p\", \"rb\"))\n",
    "rad_vel_Cr_dict = pickle.load(builtins.open(\"2022_06_23_04_01_PM_Con&GoF_PIV_RadVel_Cr_dict.p\", \"rb\"))\n",
    "rad_vel_N_dict = pickle.load(builtins.open(\"2022_06_23_04_01_PM_Con&GoF_PIV_RadVel_N_dict.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spatial Autocorrelation of Radial Velocity Componenent\n",
    "rad_vel_Cr_mean_dict={}\n",
    "rad_vel_r_bins_dict={}\n",
    "rad_vel_Cr_dict={}\n",
    "rad_vel_N_dict={}\n",
    "key_hole = list(X_dict.keys())\n",
    "for k in tqdm(range(len(key_hole)), position=0, leave=True):\n",
    "    rad_vel_Cr_mean, rad_vel_r_bins, rad_vel_Cr, rad_vel_N = spatial_coord_NanNorm(v_rho_dict[key_hole[k]], \n",
    "                                                                                    X_dict[key_hole[k]], \n",
    "                                                                                    Y_dict[key_hole[k]], 15)\n",
    "    rad_vel_Cr_mean_dict.update({key_hole[k]:rad_vel_Cr_mean})\n",
    "    rad_vel_r_bins_dict.update({key_hole[k]:rad_vel_r_bins})\n",
    "    rad_vel_Cr_dict.update({key_hole[k]:rad_vel_Cr})\n",
    "    rad_vel_N_dict.update({key_hole[k]:rad_vel_N})\n",
    "\n",
    "# Save dictionaries as pickle files\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p_\")\n",
    "pickle.dump(rad_vel_Cr_mean_dict, builtins.open(f\"{date}Con&GoF_PIV_RadVel_Cr_Mean_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_r_bins_dict, builtins.open(f\"{date}Con&GoF_PIV_RadVel_rbins_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_Cr_dict, builtins.open(f\"{date}Con&GoF_PIV_RadVel_Cr_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(rad_vel_N_dict, builtins.open(f\"{date}Con&GoF_PIV_RadVel_N_dict.p\", \"wb\"), pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Spatial Coordination for each genotype\n",
    "\n",
    "Con_Cr_mean = []\n",
    "GoF_Cr_mean = []\n",
    "\n",
    "key_hole = list(rad_vel_Cr_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='Con':\n",
    "        Con_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    elif key_hole[k].split('_')[-2]=='GoF':\n",
    "        GoF_Cr_mean.append(rad_vel_Cr_mean_dict[key_hole[k]])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "\n",
    "Con_Cr_avg = pd.DataFrame(Con_Cr_mean).T.mean(axis=1)\n",
    "Con_Cr_sem = pd.DataFrame(Con_Cr_mean).T.sem(axis=1)\n",
    "GoF_Cr_avg = pd.DataFrame(GoF_Cr_mean).T.mean(axis=1)\n",
    "GoF_Cr_sem = pd.DataFrame(GoF_Cr_mean).T.sem(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 120\n",
    "max_key = \"\"\n",
    "for key in rad_vel_r_bins_dict:\n",
    "    cur_len = len(rad_vel_r_bins_dict[key])\n",
    "    if cur_len==max_len:\n",
    "        max_key = key\n",
    "        max_len = cur_len\n",
    "print(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Local Coordination\n",
    "col_names = rad_vel_r_bins_dict['285_2021_01_26_Pos7_GoF_top']\n",
    "df = pd.DataFrame(GoF_Cr_mean)\n",
    "df.columns = col_names\n",
    "GoF_Cr_150 = df[150.0]\n",
    "\n",
    "col_names = rad_vel_r_bins_dict['329_2021_04_10_Pos25_Con_top']\n",
    "df = pd.DataFrame(Con_Cr_mean)\n",
    "df.columns = col_names\n",
    "ConGoF_Cr_150 = df[150.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GoF Spatial Coordination Figure\n",
    "GoF_Cr_fig = go.Figure()\n",
    "\n",
    "GoF_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['282_2021_01_25_Pos15_Con_bot'][0:35], y= pd.DataFrame(Con_Cr_mean).T.mean(axis=1), name='Con', line_color='#000000', error_y=dict(type='data', array = pd.DataFrame(Con_Cr_mean).T.sem(axis=1), visible=True)))\n",
    "GoF_Cr_fig.add_trace(go.Scatter(x= rad_vel_r_bins_dict['282_2021_01_25_Pos15_Con_bot'][0:35], y= pd.DataFrame(GoF_Cr_mean).T.mean(axis=1), name='GoF', line_color='#006400', error_y =dict(type='data', array = pd.DataFrame(GoF_Cr_mean).T.sem(axis=1) , visible=True)))\n",
    "GoF_Cr_fig.update_layout(title=\"Spatial Coordination (Autocorrelation of Radial Velocity)\", xaxis_title=\"Δr(µm)\", yaxis_title=\"C(Δr)\", legend_title=\"Condition\")\n",
    "GoF_Cr_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoF_RadVel_All = pd.concat([pd.DataFrame({'R Bins':rad_vel_r_bins_dict['282_2021_01_25_Pos15_Con_bot']}), \n",
    "           pd.DataFrame({'Con Radial Cr Mean':Con_Cr_avg}), \n",
    "           pd.DataFrame({'Con Radial Cr SEM':Con_Cr_sem}),\n",
    "           pd.DataFrame({'GoF Radial Cr Mean':GoF_Cr_avg}),\n",
    "           pd.DataFrame({'GoF Radial Cr SEM':GoF_Cr_sem})], axis=1)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "GoF_RadVel_All.to_excel(f'{date}_Con&GoF_mean_Radial_Cr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoF_Cr_mean_anova = []\n",
    "for col in range(0,np.shape(pd.DataFrame(GoF_Cr_mean))[1]):\n",
    "    stat, p = stats.f_oneway(pd.DataFrame(Con_Cr_mean)[col], pd.DataFrame(GoF_Cr_mean)[col])\n",
    "    ks_stat, ks_p = stats.ks_2samp(pd.DataFrame(Con_Cr_mean)[col], pd.DataFrame(GoF_Cr_mean)[col])\n",
    "    if len(pd.DataFrame(Con_Cr_mean)[col].dropna())>0 & len(pd.DataFrame(GoF_Cr_mean)[col].dropna())>0:\n",
    "        mw_stat, mw_p = stats.mannwhitneyu(pd.DataFrame(Con_Cr_mean)[col].dropna(), pd.DataFrame(GoF_Cr_mean)[col].dropna())\n",
    "    else:\n",
    "        mw_stat, mw_p = 0, 0\n",
    "    d = cohend(pd.DataFrame(Con_Cr_mean)[col], pd.DataFrame(GoF_Cr_mean)[col])\n",
    "    GoF_Cr_mean_anova.append([stat, p, ks_stat, ks_p, mw_stat, mw_p, d])\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "GoF_mean_stats = pd.DataFrame(GoF_Cr_mean_anova, columns= ['ANOVA stat', 'ANOVA p','KS Stat',' KS p','MW Stat',' MW p', 'Cohens d'])\n",
    "GoF_mean_stats.to_excel(f'{date}_Con&GoF_Cr_mean_Stats.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con=[]\n",
    "Con_names=[]\n",
    "GoF=[]\n",
    "GoF_names=[]\n",
    "\n",
    "ang=[]\n",
    "key_hole = list(angles_rot_dict.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    ang.append(np.array(angles_rot_dict[key_hole[k]])[~np.isnan(angles_rot_dict[key_hole[k]])])\n",
    "    ang[k] = ang[k][~(ang[k]==0)]\n",
    "    ang[k] = ang[k]+((3*np.pi)/4)\n",
    "    for i in range(len(ang[k])):\n",
    "        if ang[k][i] >  np.pi:\n",
    "            ang[k][i] = ang[k][i]-(2*np.pi)\n",
    "        elif ang[k][i] <  -np.pi:\n",
    "            ang[k][i] = ang[k][i]+(2*np.pi)\n",
    "    if key_hole[k].split('_')[-2]=='Con':\n",
    "        Con.append(ang[k])\n",
    "        Con_names.append(key_hole[k])\n",
    "    elif key_hole[k].split('_')[-2]=='GoF':\n",
    "        GoF.append(ang[k])\n",
    "        GoF_names.append(key_hole[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')\n",
    "Con_angles = np.concatenate(Con)\n",
    "GoF_angles = np.concatenate(GoF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "Con_angles_df = pd.DataFrame(Con_angles)\n",
    "GoF_angles_df = pd.DataFrame(GoF_angles)\n",
    "Con_angles_df.to_csv(f'{date}_Con(GoF)_angles.csv')\n",
    "GoF_angles_df.to_csv(f'{date}_GoF_angles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con = pd.DataFrame(np.degrees(np.array(Con_angles)), columns=['Direction'])\n",
    "GoF = pd.DataFrame(np.degrees(np.array(GoF_angles)), columns=['Direction'])\n",
    "Con[\"Condition\"]=\"Con\"\n",
    "GoF[\"Condition\"]=\"GoF\"\n",
    "Con_v_GoF_df = pd.concat([Con, GoF], axis=0)\n",
    "Con_v_GoF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(Con_v_GoF_df, x=\"Direction\",\n",
    "             color = \"Condition\",\n",
    "             labels = {\"Direction\":\"Vector Direction (Degrees)\"},\n",
    "             color_discrete_map = {'Con':'#BAB0AC', 'GoF':'#006400'},\n",
    "             barmode = \"overlay\", \n",
    "             nbins = 30,\n",
    "             range_x = [-180,180],\n",
    "             range_y = [0,10],\n",
    "             histnorm = \"percent\",\n",
    "             marginal = \"box\")\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_Con&GoF_Overlay_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(Con_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'Con Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['grey'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "\n",
    "fig.write_html(f'{date}_Con(GoF)_directionality_Figure.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.degrees(np.array(Yoda1_angles)), columns=['Vector Direction (Degree)'])\n",
    "\n",
    "fig = px.histogram(df, x='Vector Direction (Degree)',  \n",
    "                   title= 'GoF Directionality', \n",
    "                   histnorm= \"percent\", \n",
    "                   nbins=30, \n",
    "                   range_y=[0,10],\n",
    "                   range_x=[-180,180],\n",
    "                   color_discrete_sequence = ['green'])\n",
    "fig.update_layout(font=dict(size=18))\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_%I_%M_%p\")\n",
    "fig.write_html(f'{date}_GoF_directionality_Figure.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConGoF_dev=[]\n",
    "ConGoF_names=[]\n",
    "GoF_names=[]\n",
    "GoF_dev=[]\n",
    "ang=[]\n",
    "\n",
    "Con_GoF_dev=[]\n",
    "GoF_dev=[]\n",
    "key_hole = list(ang_dev_all.keys())\n",
    "for k in range(len(key_hole)):\n",
    "    if key_hole[k].split('_')[-2]=='GoF':\n",
    "        GoF_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        GoF_names.append(key_hole[k])\n",
    "    elif key_hole[k].split('_')[-2]=='Con':\n",
    "        ConGoF_dev.append(np.nanmean(ang_dev_all[key_hole[k]]))\n",
    "        ConGoF_names.append(key_hole[k])\n",
    "    else:\n",
    "        print(key_hole[k])\n",
    "        raise ValueError('Genotype mismatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_v_GoF_angdev = pd.concat([pd.DataFrame(ConGoF_names, columns=['Filename']),\n",
    "                        pd.DataFrame(ConGoF_dev, columns=['Con']),\n",
    "                        pd.DataFrame(GoF_names, columns=['Filename']),\n",
    "                        pd.DataFrame(GoF_dev, columns=['GoF'])], axis=1)\n",
    "Con_v_GoF_angdev.to_excel(f'{date}Con_v_GoF_AngDev.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gof_ang_dev = pd.concat([pd.DataFrame(ConGoF_dev, columns=['Con']),\n",
    "                      pd.DataFrame(GoF_dev, columns=['GoF'])], axis=1)\n",
    "gof_ang_dev_dabest = dabest.load(gof_ang_dev, idx=(\"Con\", \"GoF\"), resamples=5000)\n",
    "gof_ang_dev_dabest.cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gof_ang_dev_dabest.cohens_d.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pal={'Con':'black', 'GoF': 'green'}\n",
    "gof_ang_dev_dabest.cohens_d.plot(swarm_label='Angular Deviation',float_contrast=False, custom_palette=color_pal, raw_marker_size=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con_stack = pd.melt(Con_DV, var_name='Filename Con', value_name='Con DV')\n",
    "GoF_stack = pd.melt(GoF_DV, var_name='Filename GoF', value_name='GoF DV')\n",
    "Direc_Var_Pair = pd.concat([DMSO_stack, Yoda1_stack], axis=1)\n",
    "\n",
    "Direc_Var_dabest = dabest.load(Direc_Var_Pair, idx=(\"Con DV\", \"GoF DV\"), resamples=5000)\n",
    "Direc_Var_dabest.cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_pal={'Con DV':'black', 'GoF DV': 'red'}\n",
    "Direc_Var_dabest.cohens_d.plot(swarm_label='Direction Variance',float_contrast=False, custom_palette=color_pal, raw_marker_size=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Figures with All Data Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = '262_2020_08_19_scratch_Pos2_DMSO_bot'\n",
    "file_name = '262_2020_08_19_scratch_Pos7_Yoda1_top'\n",
    "# file_name = '312_03_25_batch66_Pos26_cKO_bot'\n",
    "# file_name = '312_03_25_batch66_Pos8_Con_top'\n",
    "# file_name = '282_2021_01_25_Pos20_GoF_top'\n",
    "# file_name = '327_2021_04_08_Pos0_GoF_top'\n",
    "# file_name = '327_2021_04_08_Pos14_Con_top'\n",
    "\n",
    "X_mode = stats.mode(X_dict[file_name],axis=0)[0][0]\n",
    "Y_mode = stats.mode(Y_dict[file_name],axis=0)[0][0]\n",
    "U_avg = np.mean(v_theta_dict[file_name], axis=0)\n",
    "V_avg = np.mean(v_rho_dict[file_name], axis=0)\n",
    "\n",
    "colors = np.arctan2(U_avg, V_avg)+((3*np.pi)/4)\n",
    "for i in range(len(colors)):\n",
    "    for k in range(len(colors[i])):\n",
    "        if colors[i][k] >  np.pi:\n",
    "            colors[i][k] = colors[i][k]-(2*np.pi)\n",
    "        elif colors[i][k] <  -np.pi:\n",
    "            colors[i][k] = colors[i][k]+(2*np.pi)\n",
    "\n",
    "norm = Normalize()\n",
    "norm.autoscale(colors)\n",
    "\n",
    "data = V_avg\n",
    "data_mean, data_std = np.nanmean(data), np.nanstd(data)\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "count = 0\n",
    "for x in range(len(data)):\n",
    "    for y in range(len(data[x])):\n",
    "        if data[x][y] < lower or data[x][y] > upper:\n",
    "            data[x][y] = np.nan\n",
    "            count=count+1\n",
    "            \n",
    "\n",
    "vectorplot = plt.figure(figsize=(10,7))\n",
    "plt.quiver(X_mode, Y_mode, U_avg, V_avg, degrees(colors), angles='xy', cmap= 'inferno', clim=[-180,180])\n",
    "plt.colorbar(label=\"Direction (Degrees)\", orientation=\"horizontal\")\n",
    "plt.show()\n",
    "\n",
    "vectorplot.savefig(f'G:/Shared drives/Keratinocytes/06_Figures/02_Modeling/Figure PIV/{file_name}_6.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ang_dev = pd.concat([pd.DataFrame(Con_cKO_dev, columns=['Control(cKO)']),\n",
    "                         pd.DataFrame(cKO_dev, columns=['cKO']),\n",
    "                         pd.DataFrame(ConGoF_dev, columns=['Control(GoF)']),\n",
    "                         pd.DataFrame(GoF_dev, columns=['GoF']),\n",
    "                         pd.DataFrame(DMSO_dev, columns=['DMSO']),\n",
    "                         pd.DataFrame(Yoda1_dev, columns=['Yoda1'])], axis=1)\n",
    "all_ang_dev_norm = all_ang_dev/np.sqrt(2) # norm so that 1 is most random\n",
    "all_ang_dev_dabest = dabest.load(all_ang_dev_norm, idx = ((\"Control(cKO)\", \"cKO\"),\n",
    "                                                      (\"Control(GoF)\", \"GoF\"),\n",
    "                                                         (\"DMSO\", \"Yoda1\")), resamples=5000)\n",
    "all_ang_dev_dabest.cohens_d\n",
    "\n",
    "color_pal = {'Control(cKO)':'grey', 'cKO': '#d1a4ff', 'DMSO':'black', 'Yoda1': 'red', 'Control(GoF)':'grey', 'GoF': '#bbffbb',}\n",
    "edge_plot = all_ang_dev_dabest.cohens_d.plot(swarm_label='Angular Deviation',float_contrast=False, custom_palette=color_pal, raw_marker_size=7);\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "edge_plot.savefig(f\"{date}_angle_deviation_all.svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ang_dev_dabest.cohens_d.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spat_coor = pd.concat([pd.DataFrame(Con_cKO_Cr_150).rename(columns={150.0:'Control(cKO)'}),\n",
    "                           pd.DataFrame(cKO_Cr_150).rename(columns={150.0:'cKO'}),\n",
    "                           pd.DataFrame(ConGoF_Cr_150).rename(columns={150.0:'Control(GoF)'}),\n",
    "                           pd.DataFrame(GoF_Cr_150).rename(columns={150.0:'GoF'}),\n",
    "                           pd.DataFrame(DMSO_Cr_150).rename(columns={150.0:'DMSO'}),\n",
    "                           pd.DataFrame(Yoda1_Cr_150).rename(columns={150.0:'Yoda1'})], axis=1)\n",
    "all_spat_coor_dabest = dabest.load(all_spat_coor, idx = ((\"Control(cKO)\", \"cKO\"),\n",
    "                                                      (\"Control(GoF)\", \"GoF\"),\n",
    "                                                         (\"DMSO\", \"Yoda1\")), resamples=5000)\n",
    "all_spat_coor_dabest.cohens_d\n",
    "\n",
    "color_pal = {'Control(cKO)':'grey', 'cKO': '#d1a4ff', 'DMSO':'black', 'Yoda1': 'red', 'Control(GoF)':'grey', 'GoF': '#bbffbb',}\n",
    "spat_coor_plot = all_spat_coor_dabest.cohens_d.plot(swarm_label='Local Coordination',float_contrast=False, custom_palette=color_pal, raw_marker_size=7);\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d_\")\n",
    "spat_coor_plot.savefig(f\"{date}_LocalCoordination_all.svg\", format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spat_coor_dabest.cohens_d.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7(PIV)",
   "language": "python",
   "name": "piv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
